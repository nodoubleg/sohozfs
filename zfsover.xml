<?xml version="1.0" encoding="utf-8"?>

<chapter xml:id="zfsover" 
	xmlns="http://docbook.org/ns/docbook"
        xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0">

<title>ZFS overview</title>
<sect1 xml:id="zfsover-1"><title>Why ZFS?</title>
	<para>
		There are many storage solutions out in the wild, for both large enterprises and SoHo environments. It is outside the scope of this guide to cover them, or even enumerate them. You can choose open source self service product or a commercial one with full paid support. Some of them build on ZFS filesystem, first introduced in Sun Solaris and now part of Oracle Solaris operating system. Open Source ones, the likes of FreeNAS, and commercial, like Nexenta.
	</para>
	<para>
		You may wonder, why should you use <indexterm><primary>ZFS</primary></indexterm> at all. What is so special about the filesystem? There are many places on the Internet that explain that in much detail, so lets keep it short. </para>
		<itemizedlist>
			<listitem><para>First of all, most important aspect from this guide's point of view, is incredibly simplified administration. Thanks to merging volume manager, raid and file system are one, there are only two commands you need use to create volumes, redundancy levels, filesystems, compression, mountpoints etc. </para></listitem>
			<listitem><para>Second reason is long proven stability track. ZFS has been first publicly released in 2005 and since then countless storage solutions have been deployed based on it. I've seen hundreds of large ZFS storages in big enterprises and I'm confident the number is hundreds if not thousands of thousand more. I've also seen small, SoHo ZFS arrays and set up some of them myself. Both worlds have witnessed great stability and scalability of the new filesystem. </para></listitem>
			<listitem><para> ZFS was designed with data integrity in mind. It comes with data integrity check, metadata checksumming, data failure detection (and in case of redundant setup possibly fixing it), automatic replacement of failed devices. </para></listitem>
			<listitem><para> ZFS scales well, with ability to add new devices, control cache and more.</para></listitem>
	</itemizedlist>
	<para>
		<indexterm><primary>btrfs</primary></indexterm>
		<indexterm><primary>GlusterFS</primary></indexterm>
		<indexterm><primary>Ceph FS</primary></indexterm>
		A good question you may ask is, why not btrfs? It was being written as an answer to and natural evolution of ZFS. All the architectural disadvantages are being taken care of. So why not got for it? Why not ceph or glusterfs, for that matter?
	</para>
	<para>
		For me it is simple. I don't know btrfs or the other two that well, as of yet. What's more important, I've not seen enough of btrfs in the field to answer questions about downsides, and lets not be naive, it has to have some. So while I am looking at them with interest, I am not yet qualified. Also, objectively, btrfs has not been tested that well and throughout, while ZFS has proven itself in countless commercial and non-commercial setups. Ceph and glusterfs are also filesystems for different scenarios. Their use cases are for distributed storage. It is possible to set up single node install for them, it's not where their abilities will shine. 
	</para>
	<para><indexterm><primary>LVM</primary></indexterm>
		For a full appreciation of how ZFS allows for greater ease of administration and flexibility, lets compare a similar mirrored setup with LVM and ZFS. LVM is now able to manage raid sets itself, thus removing the need to set <command>mdraid</command>/<command>dmraid</command> set and greatly simplifying the configuration. Still, ZFS lets you do it even easier. 
		<note>
			This is the only place in this book where I am going to compare ZFS with other storage solutions. If you want to research differences from other storage filesystems, there are enough out there, in the wild.
		</note>
	</para>
	<para><indexterm><primary>LVM</primary><secondary>physical volume</secondary></indexterm>
		<indexterm><primary>LVM</primary><secondary>physical volume group</secondary></indexterm>
		<indexterm><primary>LVM</primary><secondary>logical volume</secondary></indexterm>
		This guide is not focused on LVM, some information however needs to be presented. Logical Volume Manager needs three set of configuration elements it is going to work on. Physical Volumes (pv) - translating in our example to disks, Volume Groups (pvg) that consist of pvs and Logical Volumes (lv) which are a virtual device set on top of pvgs.
	</para>
	<para>
		Creating a Logical Volume consists of three steps:
		<itemizedlist>
			<listitem><para>Create and label disk partitions to be used by LVM</para></listitem>
			<listitem><para>Create Volume Group and add pvs there</para></listitem>
			<listitem><para>Create lv on top of pvs within pvg</para></listitem>
		</itemizedlist>
	</para>
	<para>
		Given four drives: <literal>/dev/sdb</literal>, <literal>/dev/sdc</literal>, <literal>/dev/sdd</literal> and <literal>/dev/sde</literal>, first step is presented below. First recommended command removes partition table from devices. I am not sure it is required step, but I'll follow the documentation. If you don't know what drives are available on your system, use <command>lsblk</command> command. The <option>-S</option> switch makes it print only SCSI devices:
		<screen>
	trochej@ubuntuzfs:~$ lsblk -S
	NAME HCTL       TYPE VENDOR   MODEL             REV TRAN
	sda  2:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sdb  3:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sdc  4:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sdd  5:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sde  6:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sdf  7:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sdg  8:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sdh  9:0:0:0    disk ATA      VBOX HARDDISK    1.0  sata
	sdi  10:0:0:0   disk ATA      VBOX HARDDISK    1.0  sata
	sdj  11:0:0:0   disk ATA      VBOX HARDDISK    1.0  sata
	sdk  12:0:0:0   disk ATA      VBOX HARDDISK    1.0  sata
	sdl  13:0:0:0   disk ATA      VBOX HARDDISK    1.0  sata
	sdm  14:0:0:0   disk ATA      VBOX HARDDISK    1.0  sata
	sr0  1:0:0:0    rom  VBOX     CD-ROM           1.0  ata
		</screen>
		Before running any destructive action, look at outputs from <command>fdisk <option>-l</option></command> and <command>mount</command> commands, to avoid breaking drives already in use by your system.
	<screen>	
	<computeroutput>
	[root@localhost ~]#</computeroutput> <userinput>dd if=/dev/zero of=/dev/sdb \
		bs=512 count=1</userinput>
	<computeroutput>
	1+0 records in
	1+0 records out
	512 bytes (512 B) copied, 0.0423262 s, 12.1 kB/s

	[root@localhost ~]#</computeroutput> <userinput>dd if=/dev/zero of=/dev/sdc \
		bs=512 count=1</userinput>
	<computeroutput>
	1+0 records in
	1+0 records out
	512 bytes (512 B) copied, 0.00209745 s, 244 kB/s

	[root@localhost ~]#</computeroutput> <userinput>dd if=/dev/zero of=/dev/sdd \
		bs=512 count=1</userinput>
	<computeroutput>
	1+0 records in
	1+0 records out
	512 bytes (512 B) copied, 0.00207601 s, 247 kB/s

	[root@localhost ~]#</computeroutput> <userinput>dd if=/dev/zero of=/dev/sde \
		bs=512 count=1</userinput>
	<computeroutput>
	1+0 records in
	1+0 records out
	512 bytes (512 B) copied, 0.00181138 s, 283 kB/s
	</computeroutput>
	</screen>
	<indexterm><primary>LVM</primary><secondary>pvcreate</secondary></indexterm>
		Next, <command>pvcreate</command> is used to label disks as physical devices:
	
		<screen>
	<computeroutput>	
	[root@localhost ~]#</computeroutput> <userinput>pvcreate /dev/sdb \
		/dev/sdc /dev/sdd /dev/sde</userinput>
	<computeroutput>
  	Physical volume "/dev/sdb" successfully created
  	Physical volume "/dev/sdc" successfully created
  	Physical volume "/dev/sdd" successfully created
  	Physical volume "/dev/sde" successfully created
	</computeroutput>	
</screen>

	<indexterm><primary>LVM</primary><secondary>vgcreate</secondary></indexterm>
		Next step creates logical volume group:

		<screen>
	<computeroutput>	
	[root@localhost ~]#</computeroutput> <userinput>vgcreate vg1 /dev/sdb \
		/dev/sdc /dev/sdd /dev/sde</userinput>
	<computeroutput>
  	Volume group "vg1" successfully created
	</computeroutput>
</screen>

	<indexterm><primary>LVM</primary><secondary>vgdisplay</secondary></indexterm>
		We can now confirm that the group was created and, in fact, consists of the four physical devices we intended:

<screen>
	<computeroutput>
	[root@localhost ~]#</computeroutput> <userinput>vgdisplay vg1</userinput>
	<computeroutput>
  	--- Volume group ---
  	VG Name               vg1
	System ID             
	Format                lvm2
	Metadata Areas        4
	Metadata Sequence No  1
	VG Access             read/write
	VG Status             resizable
	MAX LV                0
	Cur LV                0
	Open LV               0
	Max PV                0
	Cur PV                4
	Act PV                4
	VG Size               7.98 GiB
	PE Size               4.00 MiB
	Total PE              2044
	Alloc PE / Size       0 / 0   
	Free  PE / Size       2044 / 7.98 GiB
	VG UUID               QwkE3X-H1jO-gzAZ-JXAF-FPGQ-0D43-C06JyD
</computeroutput>
		</screen>

	<indexterm><primary>LVM</primary><secondary>lvcreate</secondary></indexterm>
	Next step involves actually creating the RAID logical volume and checking that it actually exists:

	<screen>
		<computeroutput>
	[root@localhost ~]#</computeroutput> <userinput>lvcreate -L 4000M -m1 \
		-n mirrorlv vg1</userinput>
	<computeroutput>
  	Logical volume "mirrorlv" created.

	[root@localhost ~]# lvdisplay vg1
  	--- Logical volume ---
  	LV Path                /dev/vg1/mirrorlv
	LV Name                mirrorlv
	VG Name                vg1
	LV UUID                gVQoik-R1Om-bzDG-GgVy-NA3T-SgCd-80s5RU
	LV Write Access        read/write
	LV Creation host, time localhost.localdomain, 2016-02-12 13:05:50 
	LV Status              available
	# open                 0
	LV Size                3.91 GiB
	Current LE             1000
	Mirrored volumes       2
	Segments               1
	Allocation             inherit
	Read ahead sectors     auto
	- currently set to     8192
	Block device           253:6
</computeroutput>
	</screen>
	</para>
	<para>

		<indexterm><primary>mkfs.ext4</primary></indexterm>
		Now, this is not all. Before being able to mount the volume, you first need to create a filesystem on it:

		<screen>
			<computeroutput>
	[root@localhost ~]#</computeroutput> <userinput>mkfs.ext4 /dev/vg1/mirrorlv</userinput>
	<computeroutput>
	mke2fs 1.42.9 (28-Dec-2013)
	Filesystem label=
	OS type: Linux
	Block size=4096 (log=2)
	Fragment size=4096 (log=2)
	Stride=0 blocks, Stripe width=0 blocks
	256000 inodes, 1024000 blocks
	51200 blocks (5.00%) reserved for the super user
	First data block=0
	Maximum filesystem blocks=1048576000
	32 block groups
	32768 blocks per group, 32768 fragments per group
	8000 inodes per group
	Superblock backups stored on blocks: 
		32768, 98304, 163840, 229376, 294912, 819200, 884736

	Allocating group tables: done                            
	Writing inode tables: done                            
	Creating journal (16384 blocks): done
	Writing superblocks and filesystem accounting information: done 

	[root@localhost ~]#</computeroutput> <userinput>mount /dev/vg1/mirrorlv /mnt</userinput>
	<computeroutput>
	[root@localhost ~]#</computeroutput> <userinput>mount</userinput>
	<computeroutput>
	/dev/mapper/vg1-mirrorlv on /mnt type ext4 (rw,relatime,
	seclabel,data=ordered)
</computeroutput>
		</screen>
		<emphasis>NOW</emphasis> you are done.
	</para>
	<para>
		<indexterm><primary>zpool</primary><secondary>create</secondary></indexterm>
		Lets now create configuration as above with ZFS.
		<screen>
	<computeroutput>		
	[root@localhost src]#</computeroutput> <userinput>zpool create -f datapool mirror /dev/sdb \
		/dev/sdc mirror /dev/sdd /dev/sde</userinput>
	</screen>
	We can now confirm the configuration with <command>zpool list</command> and <command>zfs list</command> commands:
	<screen>
	<computeroutput>
	[root@localhost src]#</computeroutput> <userinput>zpool list</userinput>
	<computeroutput>
	NAME     SIZE  ALLOC FREE  EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT
	datapool 3.97G  50K  3.97G     -    0%    0% 1.00x ONLINE    -

	[root@localhost src]#</computeroutput> <userinput>zfs list</userinput>
	<computeroutput>
	NAME       USED  AVAIL  REFER  MOUNTPOINT
	datapool    49K  3.84G    19K  /datapool
</computeroutput>
		</screen>
	Here. It's done. You have a pool consisting of two mirrored vdevs and a filesystem mounted at /datapool. You can confirm pool layout by running:
	<screen>
		<computeroutput>
	[root@localhost src]#</computeroutput> <userinput>zpool status datapool</userinput>
	<computeroutput>
  	  pool: datapool
 	state: ONLINE
  	  scan: none requested
	config:

		NAME          STATE     READ WRITE CKSUM
		datapool      ONLINE       0     0     0
	  		mirror-0  ONLINE       0     0     0
	    	  sdb     ONLINE       0     0     0
	    	  sdc     ONLINE       0     0     0
	  		mirror-1  ONLINE       0     0     0
	    	  sdd     ONLINE       0     0     0
	    	  sde     ONLINE       0     0     0

errors: No known data errors
</computeroutput>
	</screen>
	That's it. You are free to create filesystems within the pool to your liking or just use the original datapool filesystem.
</para>
</sect1>
<sect1 xml:id="zfsover-3"><title><indexterm><primary>ZFS</primary> <secondary>basics</secondary></indexterm>ZFS basics</title>
	<para>
			To work with ZFS you will have to understand it's technical side and bit of implementation. Lots of in field failures I have seen stemmed from the fact, that people were trying to administer or even troubleshoot ZFS filesystems without really understanding what and why they were doing. Working against the design and limitations caused few times data loss. ZFS goes great lengths to protect your data, but nothing in the world is user proof. If you try really hard - you will break it. 
		</para>
		<para>
			First thing to understand is, ZFS's great features are no replacement for backups. Snapshots, clones, mirroring, will only protect your data as long as enough of the storage is available. Even having those nifty abilities at your command, you should still do backups and test them regularly. 
		</para>
		<para>
			<indexterm><primary>ZFS</primary> <secondary>Copy on Write</secondary></indexterm>ZFS is a Copy on Write filesystem that merges together filesystem, volume manager and raid. 
			<itemizedlist>
				<listitem><para>Copy on Write means that with each change to the block of data, the data is written to a completely new location on disk. It allows for transactional and very atomic filesystem - either write occurs entirely, or is not recorded as done. That helps to keep filesystem clean and undamaged in case of power failure.</para></listitem>
				<listitem><para>Merging volume manager, raid and filesystem together means that you can easily, with one or two commands create a storage volume that has a desired level of redundancy and contains ready to use filesystem or more.</para></listitem>
			</itemizedlist>
		</para>
		<sect2 xml:id="zfsover-4"><title>Terminology</title>
		<para>
			To fully understand the features, we need to discuss some terminology for here.
			<itemizedlist>
				<listitem><para><indexterm><primary>ZFS</primary><secondary>storage pool</secondary></indexterm>Storage Pool - this is a rough equivalent of volume group from volume manager. It's a combined capacity of disk drives presented to ZFS. A pool can have one or more filesystems. Filesystems created within see all the pool capacity and can grow up to whole available space. That's one point to observe. Any one filesystem can take all the available space making it impossible for other filesystems in the same pool to grow and contain new data. One of ways to deal with it is to use space reservations and quotas.</para></listitem>
				<listitem><para><indexterm><primary>ZFS</primary><secondary>vdev</secondary></indexterm>vdev is a virtual device that can consist of one or more physical drives. vdev can be a pool or be a part of larger pool. vdev can have a redundancy level or mirror, triple mirror, raidz, raidz2 or raidz3. Even higher levels of mirror redundancy are possible, but most probably impractical and too costly.</para></listitem>
				<listitem><para>A <indexterm><primary>ZFS</primary><secondary>filesystem</secondary></indexterm>filesystem is exactly that: a filesystem create in boundaries of a pool. A ZFS filesystem can only belong in one pool, but a pool can contain more that one ZFS filesystem. ZFS filesystem can have set reservations (minimum guaranteed capacity), quotas, compression and many other properties. Filesystems can be nested. It means you can create one filesystem within other. Unless you specify otherwise, filesystem will be automatically mounted within it's parent. Topmost ZFS filesystem is called the same as a pool and automatically mounted under root directory, unless specified otherwise.</para></listitem>
				<listitem><para><indexterm><primary>ZFS</primary><secondary>snapshot</secondary></indexterm>Snapshots are a point in time taken snaps of the filesystem state. Thanks to COW semantics, they are extremely cheap in mean of disk space. Basically, creating a snapshot means recording filesystem vnodes and keeping track of them. Once data on that inode is updated (written to new place - remember it is COW), the old block of data is retained. You can access the old data view by using said snapshot. This way, they only use as much space as has been changed between snapshot time and current time. </para></listitem>
				<listitem><para>Snapshots are read only. But some people thought it would be cool, if you might mount a snapshot and make changes to it. Such read-write snapshot is called a clone. <indexterm><primary>ZFS</primary><secondary>clones</secondary></indexterm>Clones have found many uses, one of greatest to me are boot environments. With operating system capable of booting off ZFS (illumos distributions, FreeBSD), you can create a clone of your operating system and then run operations in current filesystem or in clone, ie. upgrade the system or install tricky video driver. In case you need to,  you can boot back to working environment. And it only takes as much disk space as the changes introduced. I believe boot environments will come soon to ZFS on Linux soon.</para></listitem>
				<listitem><para><indexterm><primary>ZFS</primary><secondary>dataset</secondary></indexterm>Dataset - it is a ZFS pool, filesystem, snapshot, volume, clone. It is the layer of ZFS where data can be stored and retrieved.</para></listitem>
				<listitem><para><indexterm><primary>ZFS</primary><secondary>volume</secondary></indexterm>Volume - it is a filesystem that emulates block device. It cannot be used as a typical ZFS filesystem. For all purposes it behaves like a disk device. One of uses is to export it through iSCSI or FCoE protocols, to be mounted as LUNs on remote server and then used as disks. I'd like to note here, that volumes are my least favorite use of ZFS. Many of features I like ZFS for have limited or no use for those. If you use volumes and snapshot them, you cannot easily mount them locally for file retrieval, as you would with using simple ZFS filesystem. I will elaborate on it later on.</para></listitem>
				<listitem><para><indexterm><primary>resilver</primary></indexterm>Resilvering is a process of redundant group rebuilt after disk replacement. There are many reasons you may want to replace a disk - drive becomes faulted, you decide to swap the disk for any other reason - once the new drive is added to the pool, ZFS will start to restore redundancy data to it. Here is a very obvious advantage of ZFS over traditional RAIDs. Only data is being resilvered, not whole disks. Take note however, that resilvering is a low priority operating system process. On a very busy storage system, it will take more time.</para></listitem>
			</itemizedlist>
		</para>
	</sect2>
		<figure><title>Graphical representation of a possible pool</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/zpool.jpg" align="center"/>
  			</imageobject>
  			<textobject><phrase>A ZFS pool</phrase></textobject>
		</mediaobject>
		</figure>
		<para>
			In the figure 1.1. Graphical representation of a possible pool, four disks comprise two vdevs (two disks in each vdev). Within the pool, on top of vdevs, lives a filesystem. Data are automatically balanced across all vdevs, across all disks.
		</para>
		<para>
			The Copy On Write (COW) design warrants a quick explanation, as it is a core concept enabling some essential ZFS features. Figure 1.2 presents a single block of freshly written data.
			<figure><title>A single data block</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/single_block.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>A single data block</phrase></textobject>
		</mediaobject>
		</figure>
			When the block is later modified, it is not being rewritten. Instead, ZFS writes it anew in new place on disk, as in figure 1.3. The old block is still on the disk, but ready for reuse, if free space is needed.
			<figure><title>Rewritten data block</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/two_bloks.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>Rewritten data block</phrase></textobject>
		</mediaobject>
		</figure>
			Lets assume, that before the data has been modified, system operator creates a snapshot. The DATA 1 SNAP block is being marked as belonging to the filesystem snapshot. When, later on, data is modified and written in new place, the old block location is recorded in a snapshot vnodes table. Whenever filesystem needs to be restored to the snapshot time (when rolling back or mounting snapshot), the data is reconstructed from vnodes in current filesystem, unless the data block is also recorded in the snapshot table (DATA 1 SNAP)
			<figure><title>Snapshotted data block</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/two_blocks_snapshot.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>Snapshotted data block</phrase></textobject>
		</mediaobject>
		</figure>
		</para>
		<para>
			Deduplication is entirely separate scenario. The blocks of data are being compared to already present in the filesystem and if duplicates are found, only new entry is added to DeDuplication Table, not writing the actual data to the pool.
		<figure><title>Deduplicated data block</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/ddt.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>Deduplicated data block</phrase></textobject>
		</mediaobject>
		</figure>
		</para>
	</sect1>
	<sect1 xml:id="zfsover-5"><title>ZFS limitations</title>
		<para>
		Thing to keep in mind are ZFS limitations. As with every filesystem, ZFS also has its share of weaker points. To successfully operate the storage, even not DIY one, you will need to remember them. Fortunately there are not so many. Some require active monitoring.
		<itemizedlist>
			<listitem><para>As with most filesystems, ZFS suffers terrible performance penalty when filled up to 80% and more of its capacity. It is a common problem with filesystems, but quite many users I met never heard of it. Remember, when your pool starts filling to 80% of capacity, you need to look at either expanding the pool or migrating to bigger setup.</para></listitem>
			<listitem><para>You cannot shrink the pool. It means, you cannot remove drives nor vdevs from it, once they have been added. This is something of a sore point for ZFS community. Deplix, Inc. has developed a solution to this, but is still has not been merged into main OpenZFS source tree and didn't make it to ZFS as of yet.</para></listitem>
			<listitem><para>Except for turning a single disk pool into mirrored pool you cannot change redundancy type. It means, that once you decide on a redundancy type, your only way of changing it is destroying the pool and creating anew, recovering data from backups or other location. This is quite important point. Lets consider a scenario, when you create a pool consisting of two raidz vdevs, 4 disks each, 2 TB capacity each disk. That is total of 8 disks, with total pool capacity of 6 disks: 12 TB. In raidz redundancy level one disks capacity per vdev contains parity data. If you later decide to grow your pool, you need to add exactly same vdev: 4 disk raidz, 2 TB capacity each disk. Lets now consider you go for 8 disk raidz2 pool. If you decide to grow your pool, again, you need to add next 8 disks vdev.</para></listitem>
			<listitem><para>Increasing pool size should only be done by adding a new vdevs consisting of the same disk number, the same sizes and redundancy type as existing vdevs in the pool. Exception is mirrored vdevs.</para></listitem>
			<listitem><para>This is not exactly a limitation, but it is something to take into account: each vdev's disks capacity will be used up to capacity of smallest disk in the vdev. If you decide to build a mirror or raidz, or raidz2 vdev consisting of 2 TB disks and one 1 TB disk, each disk in the vdev will be filled up to 1 TB of its capacity. It can be alleviated later. You can exchange the smaller disk for larger and let ZFS rebuild (resilver in ZFS lingo) the raid. It is not very common scenario in enterprise arrays, but in small business and home storages it is not unheard of. If you need to start with used disks of varied capacity, it's possible. Bear in mind, however, the size limit that that smallest disk will incur.
			</para></listitem>
		</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="zfsover-6"><title>Pool layout explained</title>
	<para>
		The above last point may sound convoluted, so let us demystify it. Assume that we have a pool consisting of 5 disks, all of them in RAIDZ2 configuration (rough equivalent of RAID-6). 4 disks contain data and two contain parity data. Resiliency of the pool allows for loosing up to two disks. Any number above that will irreversibly destroy filesystem and result in need for backups. 
		<figure><title>Single vdev raidz-2 pool</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/raidz2-1.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>Single vdev raidz-2 pool</phrase></textobject>
		</mediaobject>
		</figure>
		Figure 4.1 above presents the pool. While it is technically possible to create a new vdev of less or larger number of disks, with different sizes, it will almost surely result in performance issues. And remember - you cannot remove from pool once added vdevs. It means, that if you suddenly add a new vdev, say, 4 disks raidz, as in figure 4.2, you will not only compromise pool integrity, introducing a vdev with lower resiliency, but also you will introduce performance issues.
		<figure><title>Wrongly enhanced pool</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/raidz2-2.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>Single vdev turned into mirror</phrase></textobject>
		</mediaobject>
		</figure>
		The one exception of "cannot change the redundancy level" rule is single disk to mirrored and mirrored to even more mirrored. You can attach a disk to a single disk vdev and that will result in mirrored vdev (Figure 4.3). You can also attach a disk to a two-way mirror, creating a triple-mirror (Figure 4.4).
		<figure><title>Single vdev turned into mirror</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/single_vdev_zpool_attach.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>Single vdev turned into mirror</phrase></textobject>
		</mediaobject>
		</figure>
		<figure><title>Two way mirror turn three-way mirror</title>
		<mediaobject>
  			<imageobject>
    			<imagedata fileref="gfx/mirror_vdev_zpool_attach.png" align="center"/>
  			</imageobject>
  			<textobject><phrase>Two way mirror turn three-way mirror</phrase></textobject>
		</mediaobject>
		</figure>
	</para>
</sect1>
<sect1 xml:id="zfsover-8"><title>Common tuning options</title>
	<para>
		Following online tutorials you may notice number of them tells you to set two options (one pool level and one filesystem level) that are supposed to increase the speed. Unfortunately, most of them don't explain what they do and why they should work: <literal>ashift=12</literal> and <literal>atime=off</literal>.
	</para>
 	<para>		
		While the truth is, they may offer a significant performance increase, setting them blindly is a major error. As stated previously, to properly administer your storage server, you need to understand why you use options that are offered.
	</para>
	<para><indexterm><primary>ashift</primary></indexterm>
		<indexterm><primary>Advanced Layout</primary></indexterm>
		The ashift option allows to set up a physical block layout on disks. When disks capacities kept growing, at some point keeping the original block size of 512 bytes became impractical and disk vendors changed it to 4096 bytes. But for backwards compatibility reasons disks sometimes still advertise 512 block sizes. This can have adverse effect on pool performance. The ashift option was introduced in ZFS to allow manual change of block sized done by ZFS. Since it's specified as a binary shift, the value is a power, thus: 2^12 = 4096. Omitting the ashift option allows ZFS to detect the value (disk can lie about it), using value of 9 will set it to blocksize of 512. The new disk block size is called Advanced Layout (AL). 
	</para>
	<para>
		The ashift option can only be used during pool setup or when adding a new device to a vdev. Which brings another issue: if you create a pool setting up ashift and later add a disk not setting it, your performance may go awry due to mismatched ashift parameters. If you know you may have used the option or are unsure, always check it before adding new devices:
	<screen>
  	<computeroutput><indexterm><primary>zpool</primary><secondary>list</secondary></indexterm>
  	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool list</userinput>
  	<computeroutput>
	NAME SIZE  ALLOC FREE  EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT
	data 2,72T 133G  2,59T    -     3%    4% 1.00x ONLINE    -
	</computeroutput>

	<computeroutput>
	trochej@madchamber:~$</computeroutput><userinput> sudo zpool get all data</userinput>
	<computeroutput>
	NAME  PROPERTY                    VALUE                SOURCE
	data  size                        2,72T                -
	data  capacity                    4%                   -
	data  altroot                     -                    default
	data  health                      ONLINE               -
	data  guid                        7057182016879104894  default
	data  version                     -                    default
	data  bootfs                      -                    default
	data  delegation                  on                   default
	data  autoreplace                 off                  default
	data  cachefile                   -                    default
	data  failmode                    wait                 default
	data  listsnapshots               off                  default
	data  autoexpand                  off                  default
	data  dedupditto                  0                    default
	data  dedupratio                  1.00x                -
	data  free                        2,59T                -
	data  allocated                   133G                 -
	data  readonly                    off                  -
	data  ashift                      0                    default
	data  comment                     -                    default
	data  expandsize                  -                    -
	data  freeing                     0                    default
	data  fragmentation               3%                   -
	data  leaked                      0                    default
	data  feature@async_destroy       enabled              local
	data  feature@empty_bpobj         active               local
	data  feature@lz4_compress        active               local
	data  feature@spacemap_histogram  active               local
	data  feature@enabled_txg         active               local
	data  feature@hole_birth          active               local
	data  feature@extensible_dataset  enabled              local
	data  feature@embedded_data       active               local
	data  feature@bookmarks           enabled              local
	</computeroutput>
 	</screen>
 		As you may have noticed, I let ZFS auto detect the value.
	</para>
	<para>
		<indexterm><primary>smartctl</primary></indexterm>
		If you are unsure about the AL status for your drives using <command>smartctl</command> command:
		<screen>
			<computeroutput>
	[trochej@madtower sohozfs]$</computeroutput> <userinput>sudo smartctl -a /dev/sda</userinput>
	<computeroutput>
	smartctl 6.4 2015-06-04 r4109 [x86_64-linux-4.4.0] (local build)
	Copyright (C) 2002-15, Bruce Allen, Christian Franke, 
		www.smartmontools.org

	=== START OF INFORMATION SECTION ===
	Model Family:     Seagate Laptop SSHD
	Device Model:     ST500LM000-1EJ162
	Serial Number:    W7622ZRQ
	LU WWN Device Id: 5 000c50 07c920424
	Firmware Version: DEM9
	User Capacity:    500,107,862,016 bytes [500 GB]
	Sector Sizes:     512 bytes logical, 4096 bytes physical
	Rotation Rate:    5400 rpm
	Form Factor:      2.5 inches
	Device is:        In smartctl database [for details use: -P show]
	ATA Version is:   ACS-2, ACS-3 T13/2161-D revision 3b
	SATA Version is:  SATA 3.1, 6.0 Gb/s (current: 6.0 Gb/s)
	Local Time is:    Fri Feb 12 22:11:18 2016 CET
	SMART support is: Available - device has SMART capability.
	SMART support is: Enabled
			</computeroutput>
		</screen>
		You will notice that my drive has line:
		<screen>
			<computeroutput>
	Sector Sizes:     512 bytes logical, 4096 bytes physical			
			</computeroutput>
		</screen>
		It tells you that drive has physical layout of 4096 bytes, but driver advertises 512 bytes for backwards compatibility.
	</para>
	<para>
		atime can be set at any time on any file system. This option tells ZFS if it should track last access for reads. While it is being advertised on the Internet as a good performance boost, disabling this option should be given a good forethought. You should be sure that your applications that will use the data off your storage, will not rely on that functionality. After all, it is a standard filesystem property, expected also from ext4 filesystem. While it may save you few writes and CPU cycles, it may also incur penalty in decreased application usability or stability. I know for certain that some IMAP servers and clients rely on those times to be set. 
	</para>
	<para>
		On the other hand, this is a cheap way to boost the performance and can be easily turned on again. Best way to decide it research applications that you will host files for and do some testing.
	</para>
</sect1>
<sect1 xml:id="zfsover-9"><title>Deduplication</title>
<para>
	<indexterm><primary>Deduplication</primary></indexterm>
	<note>
		As a rule of thumb: don't dedupe. Just don't. If you really need to watch out for disk space, look at other ways of increasing capacity. Honest.
	</note>
	ZFS has an interesting option that spurred quite lot of interest, when first introduced. Turning deduplication on tells ZFS to keep track of data blocks. Whenever data is written to disks, ZFS will compare with blocks already in the filesystem and if finds any block identical, will not write physical data, but add some meta-information and thus save lots and lots of disk space.
</para>
<para>
	While the feature seems great in theory, in practice it turned out to be rather tricky to use it smartly. First of all, deduplication comes at a cost and it's a cost in RAM and CPU power. For each data block that is being deduplicated, your system will add an entry to DDT (deduplication tables) that exist in your RAM. Ironically, for ideally deduplicating data, the result of DDT in RAM was system ground to a halt by lack of memory and CPU power for operating system functions. A catch is, DDT are persistent. You can at any moment disable deduplication, but once deduplicated data stay deduplicated and if you run into system stability issues due to it, disabling and reboot won't help. On next pool import (mount), DDT will be loaded into RAM again. There are two ways to get rid of them: destroy the pool, create it anew and restore data or disable deduplication, and move data on the pool so it gets undeduplicated on next writes. Both options will take time, depending on the size of your data. While deduplication may save your disk space, research it carefully.
</para>
<para>
	<indexterm><primary>zpool</primary><secondary>list</secondary></indexterm>
	Deduplication ratio is by default displayed by <command>zpool list</command> command. Ratio of 1.00 means no deduplication happened:
	<screen>
  	<computeroutput>
  	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool list</userinput>
  	<computeroutput>
	NAME SIZE  ALLOC FREE  EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT
	data 2,72T 133G  2,59T    -     3%    4% 1.00x ONLINE    -
	</computeroutput>
</screen>
	<indexterm><primary>ZFS</primary><secondary>deduplication</secondary></indexterm>
	You can check the deduplication setting by querying your filesystems <option>deduplication</option> property:
	<screen>
  	<computeroutput>
  	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs get dedup data/datafs</userinput>
  	<computeroutput>
	NAME         PROPERTY  VALUE          SOURCE
	data/datafs  dedup     off            default
	</computeroutput>
</screen>
	Deduplication is a setting set per filesystem.
</para>
</sect1>
<sect1 xml:id="zfsover-10"><title>Compression</title>
<para><indexterm><primary>Compression</primary></indexterm>
	An option that saves me both disk space and add some speed is compression. There are several compression algorithms available for use by ZFS. Basically, you can tell filesystem to compress any block of data it will write to disk. With modern CPUs, you can usually add some speed by writing smaller physical data. You processors should be able to cope with packing and unpacking data on the fly. Exception can be data that compress badly, ie. mp3, jpg or video. Textual data (application logs etc.) usually play well with this option. For my personal use, I always turn it on.
</para>
<para>
	The compression can be set by on filesystem basis:
	<indexterm><primary>ZFS</primary><secondary>compression</secondary></indexterm>
	<screen>
		<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs get compression data/datafs</userinput>
	<computeroutput>
	NAME         PROPERTY     VALUE     SOURCE
	data/datafs  compression  on        local

	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs set compression=on data/datafs</userinput>
	</screen>
	Compression ratio can be found out by querying a property:
	<screen>
		<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs get compressratio data/datafs</userinput>
	<computeroutput>
	NAME         PROPERTY       VALUE  SOURCE
	data/datafs  compressratio  1.26x  
		</computeroutput>
	</screen>
</para>
<para>
	Several compression algorithms are available. If you simply turn compression on, <literal>lzjb</literal> algorithm is used. It is considered a good compromise between performance and compression ratio. Other compression algorithms available to you are listed in <command>zfs</command> man page. A new algorithm is lz4. It has higher performance and higher compression ratio than lzjb. It can only be enabled for pools that have <literal>feature@lz4_compress</literal> feature flag property:
	<screen>
		<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool get feature@lz4_compress data</userinput>
	<computeroutput>
	NAME  PROPERTY              VALUE                 SOURCE
	data  feature@lz4_compress  active                local
		</computeroutput>
	</screen>
	If the feature is enabled, you can set compression=lz4 for any given dataset. You can enable it by invoking command:
	<screen>
		<computeroutput>
	trochej@madchamber:~$ <userinput>sudo zpool set feature@lz4_compress=enabled data</userinput>
		</computeroutput>
	</screen>
</para>
</sect1>
<sect1 xml:id="zfsover-11"><title>ZFS pool state</title>
<para>
	If you look again at the listing of my pool:
	<screen>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool list</userinput>
	<computeroutput>
	NAME SIZE  ALLOC FREE  EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT
	data 2,72T 133G  2,59T    -     3%   4%  1.00x ONLINE -
	</computeroutput>
</screen>
	You will notice a column called <indexterm><primary>ZFS</primary><secondary>pool HEALTH</secondary></indexterm><emphasis>HEALTH</emphasis>. 
</para>
<para>
	It is a status of your ZFS pool. There are several other indicators that you can see here:
	<itemizedlist>
		<listitem><para><indexterm><primary>ZFS</primary><secondary>pool ONLINE</secondary></indexterm>ONLINE - the pool is healthy (there are no errors detected), it is imported (mounted in traditional filesystems jargon) and ready to use. It doesn't mean it's perfectly okay. ZFS will keep pool marked online even if some small number of I/O errors or correctable data errors occur. You should monitor other indicators also, ie. disks' health (hdparm, smartctl, lsiutil for LSI SAS controllers).</para></listitem>
		<listitem><para><indexterm><primary>ZFS</primary><secondary>pool DEGRADED</secondary></indexterm>DEGRADED - probably only applicable to redundant sets, where disks in mirror or raidz or raidz2 pools have been lost. The pool may have become non-redundant. Loosing another disk may render it corrupt. Bear in mind, that in triple-mirror or raidz2 loosing one disk doesn't render pool non-redundant.</para></listitem>
		<listitem><para><indexterm><primary>ZFS</primary><secondary>pool FAULTED</secondary></indexterm>FAULTED - a disk or a vdev is inaccessible. It means that ZFS cannot read nor write to it. In redundant configurations, a disk may be FAULTED but its vdev may be DEGRADED and accessible still. This may happen if in mirrored set one disk is lost. If you loose a top level vdev, ie. both disks in a mirror, your whole pool will be inaccessible and your pool will become corrupt. Since there is no way to restore filesystem, your options at this stage are recreating pool with healthy disks and restore from backups or seek ZFS data recovery experts. The latter is usually pretty costly option.</para></listitem>
		<listitem><para><indexterm><primary>ZFS</primary><secondary>pool OFFLINE</secondary></indexterm>OFFLINE - a device has been disabled (taken offline) by administrator. Reasons may vary but it need not mean the disk is faulty.</para></listitem>
		<listitem><para><indexterm><primary>ZFS</primary><secondary>pool UNAVAIL</secondary></indexterm>UNAVAIL - the disk or vdev cannot be opened. Effectively ZFS cannot read nor write to it. You may notice it sounds very similar to FAULTED state. The difference would mainly be that in FAULTED state, device has displayed number of errors before being marked as FAULTED by ZFS. With UNAVAIL system cannot talk to the device, possibly it went totally dead, or power supply is too weak to power all of your disks.</para></listitem>
		<listitem><para><indexterm><primary>ZFS</primary><secondary>pool REMOVED</secondary></indexterm>REMOVED - if your hardware supports it, when disk is physically removed without first removing it from pool using zpool command, it will be marked as REMOVED.</para></listitem>
	</itemizedlist>
	Checking pool health explicitly is done using zpool status and zpool status -x command:
	<screen>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool status -x</userinput>
	<computeroutput>
	all pools are healthy

	trochej@madchamber:~$ sudo zpool status
  	  pool: data
 	 state: ONLINE
  	  scan: none requested
	config:

        	NAME        STATE     READ WRITE CKSUM
        	data        ONLINE       0     0     0
          		sdb      ONLINE      0     0     0

	errors: No known data errors
	</computeroutput>
</screen>
<literal>zpool status</literal> will print detailed health and configuration of all pool devices. When pool consists of hundreds of disks, it may be troublesome to fish out a faulty device. To that end you can use <literal>zpool status -x</literal>, which will print only status of pools that experienced issues.
	<screen>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool status -x</userinput>
	<computeroutput>
  	  pool: data
 	 state: DEGRADED
  	status: One or more devices has been taken offline by the administrator.
            Sufficient replicas exist for the pool to continue functioning in a
            degraded state.
	action: Online the device using 'zpool online' or replace the device with
            'zpool replace'.
 	scrub: resilver completed after 0h0m with 0 errors on Wed Feb 10 15:15:09 2016
	config:

        	NAME        STATE     READ WRITE CKSUM
        	data        ONLINE       0     0     0
        	  mirror-0  DEGRADED     0     0     0
          		sdb      ONLINE      0     0     0
          		sdc      OFFLINE     0     0     0 48K resilvered

	errors: No known data errors
	</computeroutput>
</screen>
</para>
<para>
	Further details are out of the scope of this book. You should visit <ulink url="http://completelyfake.eu/illumos/docs/zfsadmin/">ZFS Administrator Guide</ulink> managed by illumos, OpenZFS and OpenIndiana community. You should also study <command>zfs</command> and <command>zpool</command> commands' man pages.
	</para>
</sect1>
<sect1 xml:id="zfsover-12"><title>ZFS version</title>
<indexterm><primary>ZFS</primary><secondary>version</secondary></indexterm>
<para>
	ZFS was designed with relative ease of incremental features introduction. As part of that mechanism ZFS versions have been introduced in means of a single digit. Tracking that digit, system operator would know if their pool uses latest ZFS version - including new features and bugfixes. Upgrade is done inplace and does not require any downtime.
</para>
	<para>
		That was functioning quite well, when ZFS was developed solely by Sun Microsystems. With advent of OpenZFS community - gathering developers from illumos, Linux, OSX and FreeBSD worlds - it soon became obvious, that it would be difficult if not impossible to agree with every on-disk format change across whole community. Thus the version number stayed at the latest that was ever released as opensource from Oracle Corp: 28. From that point pluggable architecture of "features flags" was introduced. ZFS implementations are compatible if they implement the same set of feature flags. 
	</para>
	<para>
		If you look again at the <literal>zpool</literal> command output for my host:
	<screen>
  	<computeroutput>
  	trochej@madchamber:~$</computeroutput><userinput> sudo zpool get all data</userinput>
	<computeroutput>
	NAME  PROPERTY                    VALUE                SOURCE
	data  size                        2,72T                -
	data  capacity                    4%                   -
	data  altroot                     -                    default
	data  health                      ONLINE               -
	data  guid                        7057182016879104894  default
	data  version                     -                    default
	data  bootfs                      -                    default
	data  delegation                  on                   default
	data  autoreplace                 off                  default
	data  cachefile                   -                    default
	data  failmode                    wait                 default
	data  listsnapshots               off                  default
	data  autoexpand                  off                  default
	data  dedupditto                  0                    default
	data  dedupratio                  1.00x                -
	data  free                        2,59T                -
	data  allocated                   133G                 -
	data  readonly                    off                  -
	data  ashift                      0                    default
	data  comment                     -                    default
	data  expandsize                  -                    -
	data  freeing                     0                    default
	data  fragmentation               3%                   -
	data  leaked                      0                    default
	data  feature@async_destroy       enabled              local
	data  feature@empty_bpobj         active               local
	data  feature@lz4_compress        active               local
	data  feature@spacemap_histogram  active               local
	data  feature@enabled_txg         active               local
	data  feature@hole_birth          active               local
	data  feature@extensible_dataset  enabled              local
	data  feature@embedded_data       active               local
	data  feature@bookmarks           enabled              local
	</computeroutput>
 	</screen>
 	You will notice that last few properties start with <emphasis>feature@</emphasis> string. That's the feature flags you need to look for. The find out the all supported versions and feature flags run commands: <literal>sudo zfs upgrade -v</literal> and <literal>sudo zpool upgrade -v</literal> as in examples below:
 	<screen>
 	<computeroutput>
 	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs upgrade -v</userinput>
 	<computeroutput>
	The following filesystem versions are supported:

	VER  DESCRIPTION
	---  --------------------------------------------------------
	 1   Initial ZFS filesystem version
	 2   Enhanced directory entries
	 3   Case insensitive and filesystem user identifier (FUID)
	 4   userquota, groupquota properties
	 5   System attributes

	For more information on a particular version, including supported 
	releases, see the ZFS Administration Guide.
	</computeroutput>

	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool upgrade -v</userinput>
	<computeroutput>
	This system supports ZFS pool feature flags.

	The following features are supported:

	FEAT DESCRIPTION
	-------------------------------------------------------------
	async_destroy                         (read-only compatible)
	     Destroy filesystems asynchronously.
	empty_bpobj                           (read-only compatible)
	     Snapshots use less space.
	lz4_compress                         
	     LZ4 compression algorithm support.
	spacemap_histogram                    (read-only compatible)
	     Spacemaps maintain space histograms.
	enabled_txg                           (read-only compatible)
	     Record txg at which a feature is enabled
	hole_birth                           
	     Retain hole birth txg for more precise zfs send
	extensible_dataset                   
	     Enhanced dataset functionality, used by other features.
	embedded_data                        
	     Blocks which compress very well use even less space.
	bookmarks                             (read-only compatible)
	     "zfs bookmark" command

	The following legacy versions are also supported:

	VER  DESCRIPTION
	---  --------------------------------------------------------
	 1   Initial ZFS version
	 2   Ditto blocks (replicated metadata)
	 3   Hot spares and double parity RAID-Z
	 4   zpool history
	 5   Compression using the gzip algorithm
	 6   bootfs pool property
	 7   Separate intent log devices
	 8   Delegated administration
	 9   refquota and refreservation properties
	 10  Cache devices
	 11  Improved scrub performance
	 12  Snapshot properties
	 13  snapused property
	 14  passthrough-x aclinherit
	 15  user/group space accounting
	 16  stmf property support
	 17  Triple-parity RAID-Z
	 18  Snapshot user holds
	 19  Log device removal
	 20  Compression using zle (zero-length encoding)
	 21  Deduplication
	 22  Received properties
	 23  Slim ZIL
	 24  System attributes
	 25  Improved scrub stats
	 26  Improved snapshot deletion performance
	 27  Improved snapshot creation performance
	 28  Multiple vdev replacements

	For more information on a particular version, including 
	supported releases, see the ZFS Administration Guide.
	</computeroutput>
 	</screen>
 		The both commands above printed information on maximum level of ZFS pool and filesystem versions and what feature flags are available for them.
 	</para>
 	<para>
 		Current version of you pool and filesystems can be checked with <literal>zpool upgrade</literal> and <literal>zfs upgrade</literal> commands:
 	<screen>
 	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool upgrade</userinput>
	<computeroutput>
	This system supports ZFS pool feature flags.

	All pools are formatted using feature flags.

	Every feature flags pool has all supported features enabled.
	</computeroutput>

	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs upgrade</userinput>
	<computeroutput>
	This system is currently running ZFS filesystem version 5.

	All filesystems are formatted with the current version.
	</computeroutput>

 	</screen>
	</para>
</sect1>
</chapter>