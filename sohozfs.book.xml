<?xml version="1.0" encoding="utf-8"?>

 <book xmlns="http://docbook.org/ns/docbook"
        xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0">

<xi:include href="info.xml"/>

<xi:include href="preface.xml"/>

<xi:include href="diy.xml" />


<xi:include href="zfsover.xml" />

<chapter xml:id="planning"><title>Planning</title>
	<para>Before you go and buy a hardware for your storage, there are few things to consider. How much disk space will you need? How many client connections (sessions) your storage will serve? Which protocol will you use? What kind of data you plan to serve?
	</para>
<sect1 xml:id="planning-2"><title>Don't rush</title>
	<para>
		First advice that you always should keep in mind: don't rush it. You are about to invest your money and time. While you can later modify the storage according to your needs, some changes will require that you recreate the ZFS pool, which means all data on it will be lost. If you buy wrong disks (example: too small), you will need to add more and may run out of free slots. 
	</para>
</sect1>
<sect1 xml:id="planning-3"><title>Questionnaire</title>
	<para>There are few questions you should ask yourself before starting to scope the storage. Answers that you will give here will play key role in later deployment.
		<itemizedlist><listitem><para>The amount of data you expect to store will determine number and size of disks you will need to buy. That will also imply other factors, like server size.</para></listitem>
		<listitem><para>Number of concurrent client connections will determine amount of RAM that's you'll need. It may also imply buying SSD disks to serve as lever 2 cache for your storage and resigning from using SATA disk at all, if you were considering them. Even if you are going to store hundreds of terabytes, but only few client machines will ever utilize it and not very intensively, you may well get with low amount of memory.</para></listitem>
		<listitem><para>The above will also imply the kind of network interface in you server and switch it should be attached to.</para></listitem>
		<listitem><para>How critical the data are should push you into looking at certified and probably more costly hardware, known to perform well and for longer time. It will also tell you which redundancy level you should use, influencing the final cost.</para></listitem>
		<listitem><para>The kind of data you will serve, may imply the architecture of you storage pool. Streaming a video files for considerably large amount of clients or service virtual machines data files will most probably mean you need to use mirrors, which directly influence final capacity of your array.</para></listitem>
		</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="planning-4"><title>Scope</title>
	<para>
		Firstly, lets create upper bounds for what will be considered a SoHo storage in this guide. Given current disk sizes, up to 12 slots in a single node, up to 30 TB of raw capacity. Either internal SAS or SATA drives. One or two slots for eventual SSDs for speeding up reads. Possibly a mirrored ZIL device to speed up and concatenate writes to disks. A system drive, possibly mirrored, although, currently setting up Linux system on ZFS is not trivial and booting from ZFS is not recommended. Up to 128 Gigabytes of RAM, possibly 64. A 64bit CPU with four or more cores. While running ZFS on 32bit systems is possible, it certainly is not recommended. If you intend to use external disk enclosures (JBODS) connected through SAS or FibreChannel, this book is most probably not intended for you. It is possible to set up and administer such storage manually and many people have done so, but it may involve additional steps not covered in the guide. If you want to run tens or hundreds of disks, you'd probably do yourself a favor if you consider FreeNAS or even commercial solutions with paid support. Keeping track of system performance, memory usage, disks, controllers and cables health is probably best managed by specialized products.
	</para>
</sect1>
<sect1 xml:id="planning-5"><title>HW buyer guide</title>
	<para>
		Hardware is usually a long term investment. Try to remember about points below:
		<itemizedlist>
			<listitem><para>When buying disks, a common practice is to make sure you buy each disk from the same vendor and model, to keep geometry and firmware the same, but from different batch, so you minimize risk of several disks dying at the same time. I suppose that for a small time buyer (few up to 20 disks) simplest way to achieve it is to buy disks from different shops. A cumbersome, but storage operators have seen disk batches failing at the same time many times in their lives.</para></listitem>
			<listitem><para>Buy few pieces for spares. Storage system lifetime is usually counted in years and is often longer than that of disk model, especially if you decide to use consumer grate SATA disks. When one of them fails in few years, you may be surprised by the fact, that you cannot buy this model any more. Introducing different one in a pool is always a performance risk. Although, if that happens, don't despair. ZFS lets you exchange all disks in a pool. This trick has been used in the past to increase size of the pool, when it became insufficient. I'll explain the procedure in Maintenance chapter. Be aware, that replacing all disks in 10 disk pool can take weeks on a filled and busy system.</para></listitem>
			<listitem><para>Scope power supply properly. If your power unit is unstable or insufficient, you may encounter mysterious failures (disks dissapearing, disk connection dropping, random i/o errors to the pool) or may not be able to use your disks at all.</para></listitem>
			<listitem><para>Performance wise, the more disks the better. The smaller disks, the better. ZFS threads writes and reads among vdevs. The more vdevs, the more read/write threads.</para></listitem>
			<listitem><para>Performance wise, plan for much RAM. ZFS needs at least 2 Gigabytes of RAM to work sensibly, but for any real life use, I'd not go below 8 Gigabytes. For a storage system for SoHo, I would recommend looking at 64 GiB and more. The thing is, ZFS caches data very aggressively, so it will try to use as much RAM as possible. It will however yield any time, system demands RAM for normal operation (new program being run etc.). So the more it can fit in your memory, the better. </para></listitem>
			<listitem><para>Plan for SSDs (at least three). You don't need to buy them upfront. ZFS is a hybrid storage filesystem, which means, it can use SSD disks for level 2 cache. It's gonna be much slower than you RAM, but it's cheaper and still much faster than your platter disks. For a fraction of RAM price you can get 512 GiB SSD drive, which should allow for another speed improvement. That's one SSD. Next two SSDs would be for external ZFS Intent Log. The filesystem don't flush all data all the time to physical storage. It ties writes in transactions and flushed several at the same time, to minimize filesystem fragmentation and real i/o to disks. If you give ZFS external device for ZIL, it can speed things up by grouping even more data before flushing it down. This additional pool device should be mirrored, because it's where you can loose your data. In case of power failure, data on external ZIL must be persistent. There are battery backed up DRAM devices that emulate small SSD disks, ie. ZeusRAM. They come in 8 and 16 GB sizes, which is enough for ZIL and are fast as memory, but they are costly. You can think of mirroring your L2ARC too (the level 2 cache), but loosing this device won't endanger your data.</para></listitem>
			<listitem><para>While SAS standard is sure to get you better performance and life expectancy from your disks, for SoHo solutions SATA is enough, especially if you consider that there are enterprise class SATA disks. The price difference however for such deployment shouldn't be very high. If unsure, choose SAS, if your budget allows.</para></listitem>
			<listitem><para>Do not buy hardware and softraid controllers. While in the past RAID cards were necessary to offload both CPU units and RAM, both of those resources are now abundant and cheap. You CPU and your RAM will be more than enough for the workload and RAID cards take away one important capability of ZFS. ZFS ensures data safety by talking directly to the disk: getting reliable information on when data i s flushed to physical disks and what block sizes are being used. RAID controllers mediate in between and can make their own "optimizations" to the I/O, which may lower ZFS reliability. Other thing is, RAID controllers are incompatible between various vendors and even the same card but different firmware revision may be unable to access your RAID set. This means, that in case of controller failure, you loose whole setup and need to restore data from backup. Softraids are even worse, in that that need a special software (often limited to only one operating system), to actually work. ZFS is superior in all of above. Not only can it use all processing power and all RAM you can give it to speed up your I/O, but also disks in the pool can be migrated between all software platforms that implement the same OpenZFS version. Also, the exact sequence of disks in disk slots is not important, as pool remembers its configuration based on both disk device names (ie. /dev/sdb) but also disk GUID given them by ZFS during pool creation.</para></listitem>
			<listitem><para>Networking cards at least 1Gb of speed. Remember, that this server networking card's bandwidth will be spread among all computer machines that will simultaneously utilize the storage. It is quite sensible to consider 10Gb, but you also need to consider your other networking gear: switches, cabling etc. Remember though: network plays a role in performance analysis at quite large amount of performance issues I was debugging were caused not by storage itself, but by networking layer.</para></listitem>
			<listitem><para>Plan for redundancy. Always. This means, that for high speed read pools you need to consider mirrored storage, effectively halving total capacity of disks you buy. RAIDZ setup means your capacity will be lowered by one disk per each vdev you create. For RAIDZ-2, it will be two disks.</para></listitem>
		</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="planning-6"><title>Data security</title>
<para>
	You are going to use ZFS storage to keep data and serve it to various people in your company. Be it two, ten or fifty, always put some thought into planning the layout. Various directories that will store data that vary by kind, sensitivity, compressibility will pay off in the future. Well designed directory structure will simplify both organizational things, like access control, and technical side, like enabled or disabled compression, atime option etc.
</para>
<para>
	ZFS filesystems behave like directories. It is quite common to create a separate ZFS filesystem per user home directory, or example, so that they can have fine grained backup policies, ACLs and compression mechanisms.
</para>
<para>
	This is one place in the guide, where I cannot be very specific. You will need to consider your company size, number of employees accessing the storage, growth perspectives, data sensitivity etc. Whatever you do, however, don't skip this point. I've seen quite a few companies, which overlooked the moment they have should switched from infrastructure that freely evolves, into something that is engineered. 
	</para>
	<sect2 xml:id="planning-7"><title>CIA</title>
	<para>
		There are many data security methodologies and one of them, I believe most classic, uses acronym C.I.A. to explain aspects of data security. Letters stand for Confidentiality, Integrity and Availability. While it focuses rather on InfoSec side of things, it's pretty good view on storage administration also. Let me introduce those in the view of storage administrator.
		<itemizedlist>
			<listitem><para>Confidentiality. Data must be available only to people that are entrusted with them. No one that is not explicitly allowed to view data, shouldn't be able to access it. This side of security is covered by many infrastructural tools, from policies and NDAs that people allowed to view data should read and sign, through network access separation (VPNs, VLANs, access control through credentials), there are also aspects directly related to storage itself: Access Control Lists (ACLs), sharing through secure protocols and in secure networks, working with storage firewall etc.</para></listitem>
			<listitem><para>Integrity. It must be guaranteed that data is genuine and was not changed by people that are not entrusted. Also, the change should not be introduced by software or hardware, intentionally or not, if it's not supposed to do it. Through whole data lifecycle, only people with sufficient privileges should be allowed to modify data. Unintentional data integrity breach may be a disk failure that breaks data blocks. While with text data it is usually easily spotted, with other data, like sound or video, that may introduce subtle differences from original state. As with all aspects of security, it's also only partially administered by storage. The data integrity is covered by ACLs, but also by ZFS checksumming data blocks to detect corruption. If your setup uses any redundancy, ZFS is able, to great extent, fix those for you using the redundant set. </para></listitem>
			<listitem><para>Availability. The data should be available at all times it is required and guaranteed. This is probably one of most obvious aspects of storage. At any time that you expect your data should be up, the data should be up. Typically storage redundancy is brought up here (mirror, raidz, raidz2), but also network cards trunking, switch stacking and redundancy of any credentials checking solution you may be using (Active Directory server, primary and secondary, for example).</para></listitem>
		</itemizedlist>
		Through this guide I will cover some of those three related to storage platform. I won't however, cover in depth administration of Linux firewall and networking, as those are subjects for separate books. Remember though, while C.I.A triad go introduced in the layout section, it applies to any other aspect of the storage. From writing access policies, through ensuring hardware is working properly, to configuring the system so that it does perform well enough so that data is available to all interested parties at any time guaranteed. 
	</para>
</sect2>
</sect1>
<sect1 xml:id="planning-8"><title>Types of workload</title>
	<para><indexterm><primary>L2ARC</primary></indexterm>
		The workload you are going to run on the storage will play major role in how you should plan the pool layout.
		<itemizedlist>
			<listitem><para>If you are going to mostly host databases and they are going to be dominating consumers of the space, L2ARC SSD device may not provide you with special performance gains. Databases are very good at caching their own data and if it so happens that the data fits into database server RAM, ARC will not have much to do. On the other hand, if the data in your database change often and will need to be reread from disks anyway, you are going to have high miss ratio anyway and again L2ARC device will not fulfill its purpose. The snapshotting data is also going to be tricky. Databases need lots more than a snapshot of filesystem to be able to work on the data. This is why they come with their own dump commands - because the full working backup will contain more than what lives in database files usually. Hosting database would usually mean you run the engine on the same host than your ZFS. Again, Database will use the RAM probably more efficiently than the filesystem itself. Consider though, if you will serve data from the same server for other purposes, like CIFS or NFS share, database and filesystem cache may compete for RAM. While they shouldn't affect the system stability, it may adversely affect he performance.</para></listitem>
			<listitem><para>If you host documents and pictures for office workers, files like procedures, technical documentations, L2ARC device is something to seriously consider. Snapshotting is then a reliable way of capturing data at certain point of time. If your data are not being accesses 24 hours a day and you can have just few seconds of off time, when no one is going to be editing them, a snapshot usually takes about a second to create and will reliably host your data at a specified point of time. You can later mount this snapshot - remember it is read only - and transfer to a backup location not worrying about data integrity.</para></listitem>
		</itemizedlist>
		Above all, don't rush it though. You can always add L2ARC later on to your pool, if performance tests prove to be unsatisfactory. 
	</para>
	<para>
		For various workload types various pool configurations are best suited. Lets first introduce terms describing storage performance:
		<indexterm><primary>IOPS</primary></indexterm>
		<indexterm><primary>storage bandwidth</primary></indexterm>
		<itemizedlist>
			<listitem><para>IOPS - it's Input/Output Operations Per Second. It is ability of hard drive or of whole storage array to write/read number of blocks per second. The IOPS needs maximum block size the disk or storage is able to write/read.</para></listitem>
			<listitem><para>Storage bandwidth is ability of disk or storage to pass amount of data per second, usually measures in MB per second.</para></listitem>
		</itemizedlist>
		Important thing to understand is that various redundancy provide various speeds for both writes and reads. ZFS pool writes are as fast as the slowest vdev and a vdev is as fast as slowest disk in it. It is due to the internal implementation of ZFS.
	</para>
	<para>
		This guide will not delve into IOPS and bandwidth mechanics. If you wish to understand it more, there are reliable online resources you can study, with <ulink url="https://en.wikipedia.org/wiki/IOPS">Wikipedia page</ulink> as a starting point.
	</para>
	<para>
		For your purposes, lets only remember, that mirrored vdevs increase read performance with each added disk. If you have two-way traditional mirrors, then your read performance is doubled. If you create a triple mirrors - it's increased three times. If you have a pool consisting of two vdevs created from three mirrored disks, then your performance can peak to a six times single disk speed. Keep in mind however, that this is given all the read data are spread between both vdevs - not uncommon, since ZFS tries to balance data spread among all vdevs it currently has. Write speed however is going to be another matter. No matter the number of disks on the pool, your performance is going to be equal the speed of slowest disk in the pool. This makes mirrors perfect for streaming or hosting virtual machines images.
	</para>
	<para>
		For RAIDZ however, the read speeds will be much slower than for mirrors. While it is not going to be exactly the speed of a single disk, the increase is going to be much slower than with mirrors. RAIDZ will however outperform mirror in writes, since each disk will only receive a chunk of data and thus the write will be done much faster than with full data set that mirrors must receive.
	</para>
</sect1>
<sect1 xml:id="planning-9"><title>Pool layout</title>
	<para>
		We've already presented various pool layout performance. Lets now consider rules of thumb for given redundancy types.
		<itemizedlist>
			<listitem><para><indexterm><primary>ZFS</primary><secondary>stripe</secondary></indexterm>There is one pool layout that was not introduced so far and for a good reason. Striped pool is a pool consisting of two or more disks that provide no redundancy. While the total pool capacity equals combined capacity of all disks within the pool, the filesystem will become corrupted and subject to data recovery given loss of a single drive. A rule of thumb for storage is: don't use stripe.</para></listitem>
			<listitem><para><indexterm><primary>ZFS</primary><secondary>mirror</secondary></indexterm>For mirrored pool a rule of thumb is, to use it only when you really need an incredible read performance or are paranoid about your storage. Reason is that disks don't fail that often and mirrors half your total pool capacity. With triple mirrors capacity will be total disks capacity divided by three. Rule of thumb - use sparingly and with care.</para></listitem>
			<listitem><para><indexterm><primary>ZFS</primary><secondary>raidz</secondary></indexterm>Rule of thumb for RAIDZ (which are a rough equivalents of RAID-5 and RAID-6) is to go rather for RAIDZ-2. It gives you quite good resilience with conserving a lot of space. There is also another recommendation and from personal experience I'd adhere to it: for RAIDZ pools have 2n+1 disks per vdev. That's three, five, seven etc., but no more than eleven. This is 2n data disks plus 1 disk for parity data. With the smallest set of three disks per vdev you have basically a capacity of mirrored set with lower read performance. Consider starting from five disks per vdev. For RAIDZ-2, the rule is to use 2x+2 disks, which translates to four, six, eight etc and have no more than twelve disk within a vdev. Given this guide considers typical target maximum of twenty disks in the pool (including ZIL and L2ARC), a good idea would be to have two eight disks RAIDZ-2 vdevs in the pool, totalling sixteen disks of total pool capacity of twelve disks. </para></listitem>
		</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="planning-10"><title>Other components to pay attention to</title>
	<para>
		It is important to pay attention to other infrastructure elements. Network would be of special interest. In a small company of few persons a small switch with workstation refit as a storage server both elements may perform without any issue, but once the number of data consumers start to grow, the network may soon turn to be a bottleneck. Switches may not be only limiting factor. Network cards in your storage server may prove to be another one. Also, if you serve your data over VPN from remote location, it may turn out the interlink is too slow. Quite often on a storage performance analysis case we were able to point networking infrastructure as a faulty element. 
	</para>
</sect1>
</chapter>
<chapter xml:id="installation">
	<title>Installation</title>
	<para>
		In this chapter we will go through basic installation of ZFS modules in your Linux distribution of choice. Most popular distributions have some kind of easy installation support for ZFS. Ubuntu has PPA that allows for quick install and setup. 
	</para>
<sect1 xml:id="installation-1"><title>System packages</title>
	<para>
		Before going any further, we need to install some packages from standard distribution repositories.
	</para>
	<sect2 xml:id="installation-1-1"><title>Virtual Machine</title>
	<para>
		Before buying the hardware and running tests on bare metal, you may wish to install and test ZFS within a virtual machine. It is a good idea and I encourage you to do so. You may in a very simple and efficient way get used to administering ZFS pools. You may also check which distribution works better for you. There are no requirements to the virtualization engine. You can use VirtualBox, VMware, KVM, Xen or any other you feel comfortable with. Keep in mind, that the tool you use should be able to provide your guest machine with virtual disks to play with. While you can create pool on files created within the VM, I don't recommend that way of testing it.
		<note>
			Bear in mind that virtual machines are not suitable for performance testing. Too many factors will stand in the way of reliable results. 
		</note>
	</para>
	</sect2>
	<sect2 xml:id="installation-1-2"><title>Ubuntu Server</title>
		<para>
			If you are running Ubuntu prior to 15.10, you will need to add special PPA repository:
	<programlisting>
				<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo add-apt-repository ppa:zfs-native/stable</userinput>
<computeroutput>
	[sudo] password for trochej: 
	 The native ZFS filesystem for Linux. Install the ubuntu-zfs package.

	Please join this Launchpad user group if you want to show support for ZoL:

	  https://launchpad.net/~zfs-native-users

	Send feedback or requests for help to this email list:

	  http://list.zfsonlinux.org/mailman/listinfo/zfs-discuss

	Report bugs at:

	  https://github.com/zfsonlinux/zfs/issues  (for the driver itself)
	  https://github.com/zfsonlinux/pkg-zfs/issues (for the packaging)

	The ZoL project home page is:

	  http://zfsonlinux.org/
	 More info: https://launchpad.net/~zfs-native/+archive/ubuntu/stable
	Press [ENTER] to continue or ctrl-c to cancel adding it

	gpg: keyring `/tmp/tmp4_wvpmaf/secring.gpg' created
	gpg: keyring `/tmp/tmp4_wvpmaf/pubring.gpg' created
	gpg: requesting key F6B0FC61 from hkp server keyserver.ubuntu.com
	gpg: /tmp/tmp4_wvpmaf/trustdb.gpg: trustdb created
	gpg: key F6B0FC61: public key "Launchpad PPA for Native ZFS for Linux" imported
	gpg: Total number processed: 1
	gpg:               imported: 1  (RSA: 1)
	OK
</computeroutput>
		</programlisting>
		</para>
		<para>
			With Ubuntu 15.10 and later, zfs support packages are already within the standard repository. You will need to install the following packages:
			<programlisting>
				<computeroutput>
				trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo apt-get install zfsutils-linux</userinput>
			</programlisting>
			That should install packages required to run zfs and compile appropriate kernel modules for you. You can later confirm they were built and in fact loaded by running <command>lsmod</command>:
			<programlisting>
	<computeroutput>trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo lsmod | grep zfs</userinput>
	<computeroutput>
	zfs                  2252800  0
	zunicode              331776  1 zfs
	zcommon                53248  1 zfs
	znvpair                90112  2 zfs,zcommon
	spl                   102400  3 zfs,zcommon,znvpair
	zavl                   16384  1 zfs

				</computeroutput>
			</programlisting>
			You should be now able to create a pool:
			<programlisting>
				<computeroutput>
					trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool create -f datapool \
					mirror /dev/sdb /dev/sdc \
					mirror /dev/sdd /dev/sde \
					mirror /dev/sdf /dev/sdg</userinput>
	trochej@ubuntuzfs:~$ sudo zpool status
	  pool: datapool
	 state: ONLINE
	  scan: none requested
	config:

		NAME        STATE     READ WRITE CKSUM
		datapool    ONLINE       0     0     0
		  mirror-0  ONLINE       0     0     0
		    sdb     ONLINE       0     0     0
		    sdc     ONLINE       0     0     0
		  mirror-1  ONLINE       0     0     0
		    sdd     ONLINE       0     0     0
		    sde     ONLINE       0     0     0
		  mirror-2  ONLINE       0     0     0
		    sdf     ONLINE       0     0     0
		    sdg     ONLINE       0     0     0

	errors: No known data errors

			</programlisting>
		</para>
	</sect2>
	<sect2 xml:id="installation-1-3"><title>CentOS</title>
		<para>
			You will need some system information tools, not installed by default, for monitoring, troubleshooting and testing of your setup:
			<programlisting>
	<computeroutput>
	[root@localhost ~]#</computeroutput><userinput> yum install sysstat</userinput>
			</programlisting>
		</para>
		<para>
			Contrary to Ubuntu, CentOS doesn't have zfs packages by default in the repository neither in 6.7 nor 7 version. Thus you need to follow <ulink url="http://zfsonlinux.org/epel.html">ZFS On Linux EPEL</ulink> directions. I have CentOS 6.7 installed. The installation for CentOS 7 is exactly the same, except for package names:
			<programlisting>
				<computeroutput>
	[root@CentosZFS ~]# yum localinstall --nogpgcheck https://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm
	[root@CentosZFS ~]# yum localinstall --nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release.el6.noarch.rpm
	[root@CentosZFS ~]# yum install -y kernel-devel zfs
				</computeroutput>
			</programlisting>
			After some time you should be ready to probe and use zfs modules:
			<programlisting>
	[root@CentosZFS ~]# modprobe zfs
	[root@CentosZFS ~]# lsmod | grep zfs
	zfs                  2735595  0 
	zcommon                48128  1 zfs
	znvpair                80220  2 zfs,zcommon
	spl                    90378  3 zfs,zcommon,znvpair
	zavl                    7215  1 zfs
	zunicode              323046  1 zfs
			</programlisting>
		</para>
		You're now ready to create a pool on your attached disks:
		<programlisting>
	[root@CentosZFS ~]# zpool create -f datapool mirror /dev/sdb /dev/sdc mirror /dev/sdd /dev/sde
	[root@CentosZFS ~]# zpool status
	  pool: datapool
	 state: ONLINE
	  scan: none requested
	config:

		NAME        STATE     READ WRITE CKSUM
		datapool    ONLINE       0     0     0
		  mirror-0  ONLINE       0     0     0
		    sdb     ONLINE       0     0     0
		    sdc     ONLINE       0     0     0
		  mirror-1  ONLINE       0     0     0
		    sdd     ONLINE       0     0     0
		    sde     ONLINE       0     0     0

	errors: No known data errors

		</programlisting>
	</sect2>
</sect1>
<sect1 xml:id="installation-2"><title>System tools</title>
	<para>
		You will need some system tools. Get used to them. 
		<itemizedlist>
			<listitem><para>We've already shown you <command>smartctl</command>. Read about this command. Not very critical, it may however provide interesting information on your disks' capabilities or their health.</para></listitem>
			<listitem><para><command>lsblk</command> will tell you what block devices you have. It will assist you in identifying drives' names you will use while setting u your ZFS pool.</para></listitem>
			<listitem><para><command>blkid</command> will help you identify drives already used by other filesystems. You may wish to use <command>mount</command> and <command>df</command> for that purpose too.</para></listitem>
</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="installation-9"><title>zfs-auto-snapshot</title>
	<para>
		One cool tool to install is ZFS Auto Snapshot: <ulink url="https://github.com/zfsonlinux/zfs-auto-snapshot">https://github.com/zfsonlinux/zfs-auto-snapshot</ulink>. I believe you should be able to follow the installation instructions from repository page. Configuration is pretty simple and consists of cron configuration files being added to your cron system service.
	</para>
</sect1>
<sect1 xml:id="installation-10"><title>arcstat.pl</title>
	<para>
		While it seems not to be a part of ZFS on Linux distribution after I installed packages, arcstat.pl will tell you important things, that will be covered later, in chapter <xref linkend="monitoring" />. Still, I consider this to be as important as other ZFS commands.
	</para>
	<para>
		You will need to start by cloning two github repositories from zfsonlinux project:
 		<programlisting>
  		<userinput>
	git clone https://github.com/zfsonlinux/linux-kstat.git

	git clone https://github.com/zfsonlinux/arcstat.git
 		</userinput>
 		</programlisting>
 		Next thing is to install their contents:
 		<programlisting>
  		<userinput>
	cd linux-kstat
	perl Makefile.PL
   	make
   	make test
   	sudo make install

   	cd ../arcstat
   	sudo mv arcstat.pl /sbin/
 		</userinput>
 		</programlisting>
 		As you can see, both are installed in a very simple and straightforward fashion. 
	</para>
</sect1>	
</chapter>
<chapter xml:id="setup"><title>Setup</title>
<sect1 xml:id="setup-1"><title>Creating a mirrored pool</title>
	<para>
		Going through this chapter should be best done at least twice. First, before you buy the hardware and secondly, when you bought it and put together, so it is ready for pool creation.
	</para>
	<para>
		Since I have shown you how to create simple pools in previous chapters, there is no need to demonstrate it now. I am therefore going to jump straight to a bit more involved configurations. Bear in mind however that with single node setup options are limited. 
	</para>
	<para>
		As a reminder, we are not going to cover striped pools at all. Your pool will have absolutely no resiliency in such a setup and you should never consider hosting data you care for on such configuration.
	</para>
	<para>
		Before running any command that may endanger your data, especially in production, ie. <command>zpool create</command> or <command>zpool destroy</command>, confirm that disks you want to use are those that you intended to be used by ZFS.
	</para>
	<para>
		<indexterm><primary>zpool</primary><secondary>create</secondary></indexterm>We have already covered a simple mirrored pool, lets create bigger one, consisting of 10 disks. I am going to follow with <command>zpool status</command> to print resulting pool configuration:
		<programlisting>
			<computeroutput>
  	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool create -f datapool mirror /dev/sdb /dev/sdc \
  	mirror /dev/sdd /dev/sde \
  	mirror /dev/sdf /dev/sdg \
  	mirror /dev/sdh /dev/sdi \
  	mirror /dev/sdj /dev/sdk
  </userinput>
  <computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool status</userinput>
	<computeroutput>
	  pool: datapool
	 state: ONLINE
	  scan: none requested
	config:

	        NAME        STATE     READ WRITE CKSUM
	        datapool    ONLINE       0     0     0
	          mirror-0  ONLINE       0     0     0
	            sdb     ONLINE       0     0     0
	            sdc     ONLINE       0     0     0
	          mirror-1  ONLINE       0     0     0
	            sdd     ONLINE       0     0     0
	            sde     ONLINE       0     0     0
	          mirror-2  ONLINE       0     0     0
	            sdf     ONLINE       0     0     0
	            sdg     ONLINE       0     0     0
	          mirror-3  ONLINE       0     0     0
	            sdh     ONLINE       0     0     0
	            sdi     ONLINE       0     0     0
	          mirror-4  ONLINE       0     0     0
	            sdj     ONLINE       0     0     0
	            sdk     ONLINE       0     0     0

	errors: No known data errors

 			</computeroutput>
 		</programlisting>
	</para>
 	<para>
 		Resulting pool total capacity equals to half the capacity of all disks in the pool:
 	<programlisting>
 		<computeroutput>
 	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool list</userinput>
 	<computeroutput>
	NAME       SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
	datapool  9.92G    64K  9.92G         -     0%     0%  1.00x  ONLINE  -
	</computeroutput>
 	</programlisting>
 		The pool already is mounted at /datapool and contains a filesystem called datapool, as you can see in the following output:

 		<programlisting>
 			<computeroutput>
 	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zfs list</userinput>
 	<computeroutput>
	NAME       USED  AVAIL  REFER  MOUNTPOINT
	datapool    58K  9.77G    19K  /datapool
	</computeroutput>
 		</programlisting>
 	</para>
</sect1>
<sect1 xml:id="setup-2"><title>Creating RAID-Z pool</title>
<para>
	<note><indexterm><primary>zpool</primary><secondary>destroy</secondary></indexterm>
		I am reusing the same disks in all examples. Before creating a new pool on them, I am going to run <command>zpool destroy</command> on the pool. It does exactly that: marks pool as destroyed and disks as free to be used by other ZFS setups. When ZFS adds a disk to the pool, it labels it with its own GUID and some information that allow ZFS to be self contained. You may move the pool around, export it from current server, reinstall the server to FreeBSD and import the same pool without a problem. Thus, if you decide you no longer need the pool and try to reuse disks for other configuration, <command>zpool</command> will refuse to add it to a new one without using <option>-f</option> switch. 
		<programlisting>
			<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool destroy datapool</userinput>
	<computeroutput>[sudo] password for trochej:</computeroutput>
		</programlisting>
	</note>
	The virtual machine I am working with has 12 disks for use as a storage:
	<programlisting>
		<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>ls -ahl /dev/sd[a-z]</userinput>
	<computeroutput>
	brw-rw---- 1 root disk 8,   0 Feb 12 21:59 /dev/sda
	brw-rw---- 1 root disk 8,  16 Feb 15 17:43 /dev/sdb
	brw-rw---- 1 root disk 8,  32 Feb 15 17:43 /dev/sdc
	brw-rw---- 1 root disk 8,  48 Feb 15 17:43 /dev/sdd
	brw-rw---- 1 root disk 8,  64 Feb 15 17:43 /dev/sde
	brw-rw---- 1 root disk 8,  80 Feb 15 17:43 /dev/sdf
	brw-rw---- 1 root disk 8,  96 Feb 15 17:43 /dev/sdg
	brw-rw---- 1 root disk 8, 112 Feb 15 17:43 /dev/sdh
	brw-rw---- 1 root disk 8, 128 Feb 15 17:43 /dev/sdi
	brw-rw---- 1 root disk 8, 144 Feb 15 17:43 /dev/sdj
	brw-rw---- 1 root disk 8, 160 Feb 15 17:43 /dev/sdk
	brw-rw---- 1 root disk 8, 176 Feb 12 21:59 /dev/sdl
	brw-rw---- 1 root disk 8, 192 Feb 12 21:59 /dev/sdm
		</computeroutput>
	</programlisting>
	<literal>/dev/sda</literal> is a system disk, that leaves me with disks from <literal>/dev/sdb</literal> to <literal>/dev/sdm</literal>. It means twelve disk for use as a storage. Lets create a RAID-Z pool following previously noted best practice of 5 disk per vdev:
	<programlisting>
		<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool create datapool \
		raidz /dev/sdb /dev/sdc \
		/dev/sdd /dev/sde /dev/sdf \
		raidz /dev/sdg /dev/sdh \
		/dev/sdi /dev/sdj /dev/sdk</userinput>
	<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool status</userinput>
	<computeroutput>
	  pool: datapool
	 state: ONLINE
	  scan: none requested
	config:

	        NAME        STATE     READ WRITE CKSUM
	        datapool    ONLINE       0     0     0
	          raidz1-0  ONLINE       0     0     0
	            sdb     ONLINE       0     0     0
	            sdc     ONLINE       0     0     0
	            sdd     ONLINE       0     0     0
	            sde     ONLINE       0     0     0
	            sdf     ONLINE       0     0     0
	          raidz1-1  ONLINE       0     0     0
	            sdg     ONLINE       0     0     0
	            sdh     ONLINE       0     0     0
	            sdi     ONLINE       0     0     0
	            sdj     ONLINE       0     0     0
	            sdk     ONLINE       0     0     0

	errors: No known data errors

	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool list</userinput>
	<computeroutput>
	NAME       SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
	datapool  19.8G   106K  19.7G         -     0%     0%  1.00x  ONLINE  -
	</computeroutput>
	</programlisting>
</para>
<para>
	Setup above can withstand loosing a single disk per each vdev at once. With two disks unused, you can add so called <indexterm><primary>ZFS</primary><secondary>hot spare</secondary></indexterm> <literal>hot-spares</literal>. Hot spares are idle disks added to a pool for replacement in case any active disk in a pool fails. The replacement is done automatically by ZFS. The hot spare mechanism isn't intelligent, so it can cause resiliency issues if you care for physical layout of your pool - spreading you pool's disks in different jbods, so that you can loose whole chassis and still retain pool and data.
	</para>
	<para>
		In a simple single server setup the problem above isn't of that significance. You should be totally safe adding the spare disk to a pool. I'll demonstrate it in <xref linkend="advsetup" /> chapter.
	</para>
</sect1>
<sect1 xml:id="setup-3"><title>Creating RAID-Z2 pool</title>
	<para>Lets now walk through creating a RAID-Z2 pool, which will consist of 12 disks spread evenly between two vdevs:
		<programlisting>
			<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool create -f datapool \
		raidz2 /dev/sdb /dev/sdc /dev/sdd \
		/dev/sde /dev/sdf /dev/sdg \
		raidz2 /dev/sdh /dev/sdi /dev/sdj \
		/dev/sdk /dev/sdl /dev/sdm</userinput>
		<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool status</userinput>
	<computeroutput>
	  pool: datapool
	 state: ONLINE
	  scan: none requested
	config:

	        NAME        STATE     READ WRITE CKSUM
	        datapool    ONLINE       0     0     0
	          raidz2-0  ONLINE       0     0     0
	            sdb     ONLINE       0     0     0
	            sdc     ONLINE       0     0     0
	            sdd     ONLINE       0     0     0
	            sde     ONLINE       0     0     0
	            sdf     ONLINE       0     0     0
	            sdg     ONLINE       0     0     0
	          raidz2-1  ONLINE       0     0     0
	            sdh     ONLINE       0     0     0
	            sdi     ONLINE       0     0     0
	            sdj     ONLINE       0     0     0
	            sdk     ONLINE       0     0     0
	            sdl     ONLINE       0     0     0
	            sdm     ONLINE       0     0     0

	errors: No known data errors

	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool list</userinput>
	<computeroutput>
	NAME       SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
	datapool  23.8G   152K  23.7G         -     0%     0%  1.00x  ONLINE  -
	</computeroutput>
		</programlisting>
	</para>
</sect1>
<sect1 xml:id="setup-10"><title>Forcing operations</title>
<para>
	There are situations where you will want to conduct two operations with very final consequences: destroying a pool or forcing an operation on a pool, ie. create. You may see lots of it especially  in first stages, when you will be learning the ZFS administration..
</para>
<para>
	Best practice is to destroy a pool before reusing its components, but there are situations when you may end up with a bunch of healthy disks disposed of by someone else. They may contain disks previously in a ZFS pool, but not enough of them to import it and destroy properly. 
	</para>
	<para>
		For such occasions there is the <option>-f</option> switch, meaning <emphasis>force</emphasis>.
	</para>
</sect1>
</chapter>
<chapter xml:id="advsetup"><title>Advanced setup</title>
<sect1 xml:id="advsetup-1"><title>Hot spares</title>
<indexterm><primary>ZFS</primary><secondary>hot spare</secondary></indexterm>
	<para>
		As mentioned previously, you can assign a hot spare disk to your pool. In case ZFS pool looses a disk, the spare will be automatically attached and <indexterm><primary>resilvering</primary></indexterm> resilvering process will be started.
	</para>
	<para>
		Lets consider a mirrored pool consisting of two vdevs two drives each:
		<programlisting>
			<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool status</userinput>
	<computeroutput>
	  pool: datapool
	 state: ONLINE
	  scan: none requested
	config:

		NAME        STATE     READ WRITE CKSUM
		datapool    ONLINE       0     0     0
		  mirror-0  ONLINE       0     0     0
		    sdb     ONLINE       0     0     0
		    sdc     ONLINE       0     0     0
		  mirror-1  ONLINE       0     0     0
		    sdd     ONLINE       0     0     0
		    sde     ONLINE       0     0     0

	errors: No known data errors
			</computeroutput>
		</programlisting>
		You add a <indexterm><primary>hot spare</primary><secondary>add</secondary></indexterm> hot spare device by running <command>zpool add</command><option>spare</option> command:
		<programlisting>
			<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool add datapool -f spare /dev/sdf</userinput>
		</programlisting>
		Confirm the disk has been added by querying the pool's status:
<programlisting>
	<computeroutput>
	trochej@ubuntuzfs:~$</computeroutput> <userinput>sudo zpool status datapool</userinput>
	<computeroutput>
	  pool: datapool
	 state: ONLINE
	  scan: none requested
	config:

		NAME        STATE     READ WRITE CKSUM
		datapool    ONLINE       0     0     0
		  mirror-0  ONLINE       0     0     0
		    sdb     ONLINE       0     0     0
		    sdc     ONLINE       0     0     0
		  mirror-1  ONLINE       0     0     0
		    sdd     ONLINE       0     0     0
		    sde     ONLINE       0     0     0
		spares
		  sdf       AVAIL   

	errors: No known data errors
			</computeroutput>
		</programlisting>
	</para>
	<para>
		<indexterm><primary>hot spare</primary><secondary>remove</secondary></indexterm>
		In a case you'd wish to remove the spare from the pool, use <command>zpool remove</command> command:
	<programlisting>
		<computeroutput>
			trochej@ubuntuzfs:~$ sudo zpool remove datapool /dev/sdf
		</computeroutput>
	</programlisting>
	As previously, you can use <command>zpool status</command> to confirm the change.
	</para>
</sect1>
<sect1 xml:id="advsetup-2"><title>ZIL device</title>
</sect1>
<sect1 xml:id="advsetup-3"><title>L2ARC device (cache)</title>
</sect1>
<sect1 xml:id="advsetup-4"><title>Quotas</title>
</sect1>
<sect1 xml:id="advsetup-5"><title>Reservations</title>
</sect1>
<sect1 xml:id="advsetup-6"><title>Snapshots and clones</title>
</sect1>
<sect1 xml:id="advsetup-7"><title>ACL-s</title>
</sect1>
<sect1 xml:id="advsetup-8"><title>Pool properties</title>
</sect1>
<sect1 xml:id="advsetup-9"><title>ZFS Send and Receive</title>
</sect1>
<sect1 xml:id="advsetup-10"><title>Passwordless ZFS commands</title>
	<para>
		For both monitoring that will be covered later on and for day to day operations, it is useful to be able to run some commands without typing in your password with every sudo invocation. To that end, I have prepared a sudoers file based on a sudoers file installed by default with ZOL packages on Fedora. 
	</para>
	<para>
		First assumption is, you don't want everyone to run those commands passwordless. Create a special system group to easily add the right to any system operator:
		<programlisting>
  		<userinput>
	sudo groupadd zfsadmin
 		</userinput>
 		</programlisting>
 		Then add a user to this group (my user login is trochej):
 		<programlisting>
  		<userinput>
	usermod -ag zfsadmin trochej
 		</userinput>
 		</programlisting>
 		Place a file from listing below, or download it from <ulink url="http://completelyfake.eu/zfs-sudoers">my site</ulink> and place in <literal>/etc/sudoers.d/</literal> :
		<programlisting>
  		
	# Allow read-only ZoL commands to be called through sudo
	# without a password. Remove the first '#' column to enable.
	#
	# CAUTION: Any syntax error introduced here will break sudo.
	#
	# Cmnd alias specification
	Cmnd_Alias C_ZFS = \
	  /sbin/zfs "", /sbin/zfs help *, \
	  /sbin/zfs get, /sbin/zfs get *, \
	  /sbin/zfs list, /sbin/zfs list *, \
	  /sbin/zpool "", /sbin/zpool help *, \
	  /sbin/zpool iostat, /sbin/zpool iostat *, \
	  /sbin/zpool list, /sbin/zpool list *, \
	  /sbin/zpool status, /sbin/zpool status *, \
	  /sbin/zpool upgrade, /sbin/zpool upgrade -v, \
	  /sbin/arcstat.pl

	Runas_Alias R_ROOT = root

	# allow users in zfsadmin group to use basic read-only ZFS commands
	%zfsadmin ALL = (R_ROOT) NOPASSWD: C_ZFS
 		
 		</programlisting>
 		The <literal>arcstat.pl</literal> script was added in previous steps.
	</para>
	<para>
		After placing it in <literal>/etc/sudoers.d/</literal> run visudo command and make sure the last line is:
 		<programlisting>

	#includedir /etc/sudoers.d

 		</programlisting>
 		Last thing to do, is setting up proper file attributes:
 		<programlisting>
		<userinput>
	chmod 0440 /etc/sudoers.d/zfs-sudoers
 		</userinput>
 		</programlisting>
	</para>
	<para>
		 Next time user trochej logs in, they will be able to run informational zfs and zpool subcommands without need to type in their password. Other commands, that change filesystem and pool state, still need confirmation by typing in their password.
	</para>
</sect1>
</chapter>


<chapter xml:id="sharing"><title>Sharing</title>
<sect1 xml:id="sharing-1"><title>NFS - builtin</title>
</sect1>
<sect1 xml:id="sharing-2"><title>NFS - Linux server</title>
</sect1>
<sect1 xml:id="sharing-3"><title>CIFS - builtin</title>
</sect1>
<sect1 xml:id="sharing-4"><title>CIFS - the SAMBA server</title>
</sect1>
<sect1 xml:id="sharing-5"><title>Other sharing protocols</title>
	<para>
		FC, FCoE, iSCSI.
	</para>
</sect1>
</chapter>


<chapter xml:id="accounting"><title>Space accounting</title>
<sect1 xml:id="accounting-1"><title>Understanding space accounting in ZFS</title>
<para>
	Due to rich feature set, some relying on filesystem organization, the likes of clones, snapshots and compression, space monitoring needs to be done differently from traditional Linux filesystems. The usual <command>df <option>-h</option></command> command known to each Linux server administrator is no longer sufficient and may even be misleading. You should learn to use two commands and understand their arguments and output to keep track of your free space: <command>sudo zpool list</command> and <command>sudo zfs list</command>, which, on my home workstation commands will produce output as below:
 	<programlisting>
  	<computeroutput>
  	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool list</userinput>
  	<computeroutput>
	NAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
	data  2,72T   147G  2,58T         -     3%     5%  1.00x  ONLINE  -
	</computeroutput>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list</userinput>
	<computeroutput>
	NAME          USED  AVAIL  REFER  MOUNTPOINT
	data          147G  2,53T    96K  /data
	data/datafs   147G  2,53T   147G  /mnt/data
	</computeroutput>
	</programlisting>
	This may come as a surprise, but the list about is not complete. This is because list by default omits snapshots. Remember, snapshots consume space increasingly with time, as data change on the snapshotted system. One of very often issues raised by new ZFS storage operators was that they were unable to delete data due to lack of space. They were baffled by both the fact, that deleting data wouldn't increase the available space and that consumed space in ZFS list wouldn't add up to total space available int the pool.
</para>
<para>
	It is sometimes a bit difficult to understand what consumes your pool space. I will try to explain on examples, but nothing beats experience. Create a pool, fill it with data, run snapshots, delete, create reservations. All the time observe <command>zfs list<option> -t all -o snapshot</option></command> and <command>zfs list <option>-t all</option></command> to better understand the space accounting.
	</para>
<para>
	Lets consider a situation when you have a 3 TB pool.
 	<programlisting>
  	<userinput>
	sudo zpool create datapool mirror /dev/sdb /dev/sdc
	sudo zfs create datapool/data
 	</userinput>
 	</programlisting>
 	After successful import of 2 TB of backed up data, you decide to create snapshot, so that users mistakenly deleting data wouldn't make you rerun the backup restore.
 	 <programlisting>
  	<userinput>
	sudo zfs snapshot datapool/data@after-backup-restore
 	</userinput>
 	</programlisting>
 	Take a note that running this snapshot is instantaneous and takes no disk space initially.
 </para>
 <para>
 	As it sometimes happen, just after you ran the snapshot, a user with very wide access rights (CEO maybe?), really deletes whole 2 TB of data. But, the delete job stops short of 1 TB with information, that it cannot delete more, due to the lack of space. How is that even possible? Answer is: the snapshot.
</para>
<para>
	Lets first observe filesystem on my workstation:
	<programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list</userinput>
	<computeroutput>
	NAME          USED  AVAIL  REFER  MOUNTPOINT
	data          134G  2,55T    96K  /data
	data/datafs   134G  2,55T   134G  /mnt/data
 	</computeroutput>
 	</programlisting>
 	Now, let me create a snapshot there:
	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs snapshot data/datafs@testsnapshot</userinput>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       134G  2,55T    96K  /data
	data/datafs                134G  2,55T   134G  /mnt/data
	data/datafs@testsnapshot      0      -   134G  -
 	</computeroutput>
 	</programlisting>
		Let me now upload a CentOS 7 GB iso file to the <literal>/mnt/data</literal>:
 	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       141G  2,54T    96K  /data
	data/datafs                141G  2,54T   134G  /mnt/data
	data/datafs@testsnapshot  7,14G      -   134G  -
 	</computeroutput>
 	</programlisting>
 	Notice that the snapshot size has increased up to the newly introduced data. Let me now delete whole directory containing archived ISOs:
 	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       141G  2,54T    96K  /data
	data/datafs                141G  2,54T   109G  /mnt/data
	data/datafs@testsnapshot  32,0G      -   134G  -
 	</computeroutput>
 	</programlisting>
 	What you will see is that while the REFER size for the data/datafs ZFS filesystem has shrunk, the overall USED stays the same and snapshot size has increased up to 32 GB. For comparison, lets have a look at the <command>df <option>-h</option></command> command (I have removed non-ZFS filesystems from output for clarity:

 	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>df -h</userinput>
	<computeroutput>
	Filesystem      Size  Used Avail Use% Mounted on
	data            2,6T  128K  2,6T   1% /data
	data/datafs     2,7T  109G  2,6T   5% /mnt/data
 	</computeroutput>
 	</programlisting>
 	Let me now remove some more data from datafs, just to increase the size of the snapshot:
 	<programlisting>
 	<computeroutput>
 	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
 	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       141G  2,54T    96K  /data
	data/datafs                141G  2,54T  23,3G  /mnt/data
	data/datafs@testsnapshot   117G      -   134G  -

	trochej@madchamber:~$</computeroutput> <userinput>df -h</userinput>
	<computeroutput>
	Filesystem      Size  Used Avail Use% Mounted on

	data            2,6T  128K  2,6T   1% /data
	data/datafs     2,6T   24G  2,6T   1% /mnt/data
 	</computeroutput>
 	</programlisting>
 	As you may notice, there is not much to be gleaned from the du command. It more or less tracks the space usage, but it tells nothing about the pattern. The zfs list on the other hand, tells you quite a lot. By this output alone you can see, that while your filesystem used space has shrunk, the overall used stays the same, it's just moved into another dataset's location. </para>
 	<para>
 		The <command>zfs</command> can provide you with even deeper understanding of how the space is distributed among your data. And while it's not very interesting in the case of the small experiment I've been running so far, I'll provide you with more complicated examples in just a moment. First, howewer, lets check out another option to the <command>zfs list</command>:
 	 	 <programlisting>
  			<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all -o space</userinput>
	<computeroutput>
	NAME                      AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD
	data                      2,54T   141G         0     96K              0       141G
	data/datafs               2,54T   141G      117G   23,3G              0          0
	data/datafs@testsnapshot      -   117G         -       -              -          -
 	</computeroutput>
 	</programlisting>
 	More detailed explanation of -o space follows in next section. 
</para>
<para>
	It should be now pretty clear where the issue with data deletion came from. Since the 3TB pool is capable of keeping more or less the amount of data (modulo data compression), introducing deletion of 2 TB of data on a filesystem that already holds 2 TB results in pool space running out, since the pool needs to add data to snapshot as the user keeps removing them.
</para>
<para>
	As a side note, I was whole the time working on my production data, and while I keep backups, I was confident enough in the snapshot to let me do this:
	<programlisting>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs rollback data/datafs@testsnapshot</userinput>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       134G  2,55T    96K  /data
	data/datafs                134G  2,55T   134G  /mnt/data
	data/datafs@testsnapshot     8K      -   134G  -
	</computeroutput>
	</programlisting>
</para>
</sect1>
<sect1 xml:id="accounting-2"><title>zfs -o space</title>
<para><command>zfs list <option>-o space</option></command> command and its output is important enough to warrant it a separate subsection in this guide. 
</para>
<para>
The example we went through above is pretty simple. Not much is happening on this pool and not many additional features were used. I will then create a sample ZFS pool using files storage (files emulating real block devices) and we will play few scenarios to see how setting up various ZFS properties affects available space and the zfs -o space output.
</para>
<para>
	But first, lets go through all the columns in the output and what they mean to operator:
	<itemizedlist>
		<listitem><para>AVAIL - available - total available space in the filesystem</para></listitem>
		<listitem><para>USED - used - total used space in the filesystem</para></listitem>
		<listitem><para>USEDSNAP - usedbysnapshots – the disk space used by snapshots of the dataset. This space would be freed once all snapshots of the dataset are destroyed. Since multiple snapshots can reference the same blocks, this amount may not be equal to the sum of all snapshots used space</para></listitem>
		<listitem><para>USEDDS - usedbydataset – the disk space used by the dataset itself. This disk space would be freed if: all snapshots and refreservation s of this dataset were destroyed and then the dataset itself would be destroyed</para></listitem>
		<listitem><para>USEDREFRESERV - usedbyrefreservation – the disk space used by a refreservation set on the dataset. This space would be freed once refreservation is removed</para></listitem>
		<listitem><para>USEDCHILD - usedbychildren – the disk space used by children of the dataset. This space would be freed after destroying children of given dataset</para></listitem>
	</itemizedlist>
	To calculate the USED property by hand, follow equation below:
	<literal>USED = USEDCHILD + USEDDS + USEDREFRESERV + USEDSNAP</literal>
</para>
	<para>
		The <command>zfs <option>-o space</option></command> is not very informative and interesting in the very simple example above. Consider however a following configuration:
		<itemizedlist>
			<listitem><para>A pool named datapool with RAID-Z2 redundancy</para></listitem>
			<listitem><para>5 filesystems within, two of which have regular snapshots taken each hour and retained for two weeks. Every Saturday a snapshot is taken, retained for a month.</para></listitem>
			<listitem><para>2 of filesystems above have quota set</para></listitem>
			<listitem><para>1 filesystem have set reservations</para></listitem>
			<listitem><para>1 zvol created</para></listitem>
		</itemizedlist>
		Lets the this configuration in print:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zpool list
	NAME       SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
	datapool  23.8G  5.37G  18.4G         -    14%    22%  1.00x  ONLINE  -
		</programlisting>
		So the pool says, there is above 18 Gigabytes space free in the pool. Lets look closer:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list
	NAME              USED  AVAIL  REFER  MOUNTPOINT
	datapool         13.2G  2.41G  34.0K  /datapool
	datapool/first   3.58G  6.83G  3.58G  /datapool/first
	datapool/five    50.0K  2.41G  32.0K  /datapool/five
	datapool/fourth  50.0K  2.41G  32.0K  /datapool/fourth
	datapool/second  50.0K  2.41G  32.0K  /datapool/second
	datapool/third   50.0K  2.41G  32.0K  /datapool/third
	datapool/vol01   5.16G  7.57G  16.0K  -
		</programlisting>
		But not exactly. Shouldn't AVAIL number be the same as FREE in <command>zpool list</command> output? After all it is being said, ZFS filesystems can grow up to the pool's capacity. Lets list <emphasis>all</emphasis> datasets then:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list -t all
	NAME                               USED  AVAIL  REFER  MOUNTPOINT
	datapool                          13.2G  2.41G  34.0K  /datapool
	datapool/first                    3.58G  6.84G  3.58G  /datapool/first
	datapool/first@2016-02-17-14:55   18.0K      -  32.0K  -
	datapool/first@2016-02-17-15:04       0      -  3.58G  -
	datapool/five                     50.0K  2.41G  32.0K  /datapool/five
	datapool/five@2016-02-17-14:55    18.0K      -  32.0K  -
	datapool/fourth                   50.0K  2.41G  32.0K  /datapool/fourth
	datapool/fourth@2016-02-17-14:55  18.0K      -  32.0K  -
	datapool/second                   50.0K  2.41G  32.0K  /datapool/second
	datapool/second@2016-02-17-14:55  18.0K      -  32.0K  -
	datapool/third                    50.0K  2.41G  32.0K  /datapool/third
	datapool/third@2016-02-17-14:55   18.0K      -  32.0K  -
	datapool/vol01                    5.16G  7.57G  16.0K  -
		</programlisting>
		Okay. So there are snapshots in play, so it might have taken some of the capacity, but still, why numbers are different among the datasets? Lets first explain the <literal>REFER</literal> column in <command>zfs list</command> output. It states, how much space the dataset is keeping references to. See that in the output above:
		<programlisting>
	datapool/first@2016-02-17-15:04       0      -  3.58G  -
		</programlisting>
		<literal>USED</literal> column is zero, but <literal>REFER</literal>is above 3.5G. That is typical to snapshots. Since the creation of the snapshot no change was introduced to the filesystem datapool/first, thus snapshot does not use any space at the moment. But it keeps references to 3.5 G of data that datapool/first contained at the time of snapshotting. Lets make it use some space now by removign a piece of data I copied over to the datapool:
		<programlisting>
	trochej@ubuntuzfs:~$ rm /datapool/first/Fedora-Live-KDE-x86_64-23-10.iso 
		</programlisting>
	Check how it looks like right now:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list
	NAME                               USED  AVAIL  REFER  MOUNTPOINT
	datapool                          14.7G   930M  34.0K  /datapool
	datapool/first                    9.50G  4.91G   741M  /datapool/first
		</programlisting>
		So, the filesystem datapool/first consumes 9.5G of space, fur references 741M only? Where is the rest of the claimed space consumption? First, run zfs list -t all, to see not only filesystems, but snapshots also:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list -t all
	NAME                               USED  AVAIL  REFER  MOUNTPOINT
	datapool                          14.7G   930M  34.0K  /datapool
	datapool/first                    9.50G  4.91G   741M  /datapool/first
	datapool/first@2016-02-17-14:55   18.0K      -  32.0K  -
	datapool/first@2016-02-17-15:04   18.0K      -  3.58G  -
	datapool/first@2016-02-17-15:22   1.20G      -  5.50G  -
	datapool/first@2016-02-17-15:27       0      -   741M  -

	trochej@ubuntuzfs:~$ ls -ahl /datapool/first/
	total 741M
	drwxr-xr-x 2 trochej trochej    3 Feb 17 15:25 .
	drwxr-xr-x 7 trochej trochej    7 Feb 17 14:51 ..
	-rw-r----- 1 trochej trochej 741M Feb 17 15:21 FreeBSD-11.0-CURRENT-amd64-20151130-r291495-disc1.iso
		</programlisting>
		Okay. So the filesystem holds 741M of data, but its snapshots consume 1.20GB of space. That's more like it. Still, where's the rest of my space gone?
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list -t all -o space
	NAME                              AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD
	datapool/first                    4.91G  9.50G     4.78G    741M             4G          0
	datapool/first@2016-02-17-14:55       -  18.0K         -       -              -          -
	datapool/first@2016-02-17-15:04       -  18.0K         -       -              -          -
	datapool/first@2016-02-17-15:22       -  1.20G         -       -              -          -
		</programlisting>
		The output is cut out for brevity. The datapool/first filesystem consumes 4.78G in snapshots. 4G is used by refreservation property set on the filesystem, giving it 4G of free space at cost of other filesystems. Reservations have been explained in <xref linkend="advsetup-5" />.

	</para>
</sect1>
</chapter>


<chapter xml:id="monitoring"><title>Monitoring</title>
	<para>
		Your success in hosting DIY storage will greatly depend on your ability to monitor the storage. You have very rich choice of software, from opensource free like <ulink url="https://www.nagios.org/">Nagios</ulink> or it's fork <ulink url="https://www.icinga.org/">Icinga</ulink>, <ulink url="http://www.zabbix.com/">Zabbix</ulink> to completely commercial, like <ulink url="http://www-03.ibm.com/software/products/en/ibm-workload-automation">IBM Tivoli</ulink> or <ulink url="http://www.accelops.com/">AccelOps</ulink>. A good comparison guide is always available <ulink url="https://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems">on Wikipedia</ulink>. If you still don't have a monitoring solution, implement it. If your company's infrastructure is still in its infancy, plan for monitoring and implement it as one of first services. This may be cumbersome, but it really pays off later, when the infrastructure matures. The later you start implementing monitoring, the more work you need to put into it.
	</para>
	<para>
		I will show implementing ZFS monitoring using popular monitoring suites:
		<itemizedlist>
			<listitem><para><ulink url="https://www.icinga.org/">Icinga</ulink></para></listitem>
			<listitem><para><ulink url="https://www.zabbix.com/">Zabbix</ulink></para></listitem>
			<listitem><para><ulink url="http://www.opennms.org/">OpenNMS</ulink></para></listitem>
		</itemizedlist>
		Icinga configuration should be applicable to Nagios without any modifications.
	</para>
	<sect1 xml:id="monitoring-1"><title>What to monitor?</title>
	<para>
		One of things to monitor is state of your Aadaptive Replacement Cache (ARC). It is your read cache for the filesystem. The more it stores in RAM, the lower the miss ratio, the better your performance - you will satisfy more requests from memory, totally omitting the need for touching platters of your disks.
		<programlisting>
		<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo arcstat.pl</userinput>
	<computeroutput>
    time        read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  arcsz     c  
	10:21:15     0     0      0     0     0     0    0     0    0    8.5G   8.5G 
		</computeroutput>
		</programlisting>
	</para>
	</sect1>
</chapter>


<chapter xml:id="maintenance"><title>Storage maintenance</title>
<sect1 xml:id="maintenance-1"><title>Aspects of maintenance</title>
<para>
	Storage maintenance, especially DIY one, is a very wide subject. It is both technical and non-technical. In this chapter I will try to cover as much technical side as possible and as much non-technical as is sane. Surely, real non-technical side warrants a separate book on its own.
</para>
<para>
	Typical maintenance tasks start with monitoring the storage. You need to cover hardware health, software updates for your distribution, security issues, atypical access logs (intrusion detection? access privileges breach?). Many of those are outside the scope of the book.
</para>
<para>
	Monitoring is already covered in the chapter <xref linkend="monitoring" />. 
	</para>
</sect1>
<sect1 xml:id="maintenance-2"><title>Working with hot spares</title>
<indexterm><primary>hot spare</primary><secondary>working with</secondary></indexterm>
<para>
	You've seen how to add a hot spare to the pool in <xref linkend="advsetup" /> chapter. But there is a bit more to them than that. 
</para>
</sect1>
<sect1 xml:id="maintenance-3"><title>Replacing bad drive</title>
	<para>
		Replacing a drive.
	</para>
</sect1>
<sect1 xml:id="maintenance-4"><title>Recovering destroyed pool</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-5"><title>Expanding pool</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-6"><title>Increasing capacity by disks replacement</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-7"><title>Turning single disk pool into mirrored</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-8"><title>Turning two-way mirror into three-way mirror</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-9"><title>Turning devices online and taking them offline</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-10"><title>Exporting and importing pools</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-11"><title>Manual scrub</title>
	<para>
	</para>
</sect1>
<sect1 xml:id="maintenance-12"><title>Ditto blocks</title>
	<para>
	</para>
</sect1>
</chapter>


<chapter xml:id="tracing"><title>Troubleshooting</title>
<sect1 xml:id="tracing-1"><title>I got performance issues. Where to start?</title>
	<para>
		Tracing performance issues may prove tricky. There are many items that need to be taken into account while discussing performance and not many of them are easy to capture and quantify. From users' experience and perception of what's low and fast, to the various elements between storage disks and pool space consumers, there is whole land of questions that need to be asked and many of them don't have an easy answer. Fortunately for you, your setup is small and the amount of things to check limited.
	</para>
	<para>
	For an rough overview of I/O on your pool, run <command>zpool iostat</command>:
	<programlisting>
[root@localhost ~]# zpool iostat
               capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
datapool    68.5K  3.97G      0      0      0     12

		</programlisting>

	For more detailed breakthrough call <command>zpool iostat</command> <option>-v</option>:
		<programlisting>
[root@localhost ~]# zpool iostat -v
capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
datapool    68.5K  3.97G      0      0      0     12
  mirror      26K  1.98G      0      0      0      4
    sdb         -      -      0      0     62    198
    sdc         -      -      0      0     62    198
  mirror    42.5K  1.98G      0      0      0      8
    sdd         -      -      0      0     62    202
    sde         -      -      0      0     62    202
----------  -----  -----  -----  -----  -----  -----
	</programlisting>
<programlisting>
	[root@localhost ~]# vmstat
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 3018652   1832 711616    0    0    60    27   21   72  0  0 99  0  0
[root@localhost ~]# iostat
Linux 3.10.0-327.4.5.el7.x86_64 (localhost.localdomain) 	02/12/2016 	_x86_64_	(1 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.21    0.00    0.18    0.15    0.00   99.46

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda               2.83        58.99        26.55     549131     247144
sdb               0.05         0.32         0.17       2984       1545
sdc               0.04         0.27         0.17       2516       1545
sdd               0.04         0.27         0.17       2520       1571
sde               0.04         0.27         0.17       2508       1571
dm-0              0.02         0.17         0.00       1544          0
dm-1              2.64        52.92        19.94     492670     185663
</programlisting>

	</para>
</sect1>
<sect1 xml:id="tracing-10"><title>zdb</title>
	<para>
		ZFS comes with a nice command that is largely undocumented. You wouldn't use it very often, but in some rare occasions it proved quite useful in the field and may gain you additional insight into inner workings of your pool: <indexterm><primary><command>zdb</command></primary></indexterm>. The output of the command looks similar to:
		<programlisting>
	[root@localhost ~]# zdb
datapool:
    version: 5000
    name: 'datapool'
    state: 0
    txg: 4
    pool_guid: 8560869201706480032
    errata: 0
    hostname: 'localhost.localdomain'
    vdev_children: 2
    vdev_tree:
        type: 'root'
        id: 0
        guid: 8560869201706480032
        create_txg: 4
        children[0]:
            type: 'mirror'
            id: 0
            guid: 6918291420727026689
            metaslab_array: 37
            metaslab_shift: 24
            ashift: 9
            asize: 2132279296
            is_log: 0
            create_txg: 4
            children[0]:
                type: 'disk'
                id: 0
                guid: 5135146604379500971
                path: '/dev/sdb1'
                whole_disk: 1
                create_txg: 4
            children[1]:
                type: 'disk'
                id: 1
                guid: 8252522761405928242
                path: '/dev/sdc1'
                whole_disk: 1
                create_txg: 4
        children[1]:
            type: 'mirror'
            id: 1
            guid: 4411873599133245467
            metaslab_array: 34
            metaslab_shift: 24
            ashift: 9
            asize: 2132279296
            is_log: 0
            create_txg: 4
            children[0]:
                type: 'disk'
                id: 0
                guid: 12277352387662817231
                path: '/dev/sdd1'
                whole_disk: 1
                create_txg: 4
            children[1]:
                type: 'disk'
                id: 1
                guid: 12555207356901799735
                path: '/dev/sde1'
                whole_disk: 1
                create_txg: 4
    features_for_read:
        com.delphix:hole_birth
        com.delphix:embedded_data
		</programlisting>
	</para>
</sect1>
</chapter>
<chapter xml:id="reading"><title>Additional sources of ZFS information</title>
<para>
	While I tried for this guide to be detailed and informative, it is impossible to explore all the corners of ZFS and storage more so. Situations where you will not find an answer here will happen from time to time and it is a good thing to know where to start looking for answer. You may also wish for more knowledge or feel that I explained things too little. Places to visit are::
	<itemizedlist>
		<listitem><para><ulink url="http://open-zfs.org/wiki/Documentation"><citetitle>Open-ZFS Project Documentation</citetitle></ulink> page should probably be the first place to go. It hosts number of links to additional sources of information, as well as many good articles itself</para></listitem>
		<listitem><para><ulink url="http://zfsonlinux.org/"><citetitle>ZFS On Linux</citetitle></ulink> page. It is the ZFS porting project. It contains links to packages, distribution repositories, distribution documentations and a FAQ, as well as issue tracker</para></listitem>
		<listitem><para><ulink url="http://www.solarisinternals.com/wiki/index.php/ZFS_Evil_Tuning_Guide"><citetitle>ZFS Evil Tuning Guide</citetitle></ulink>. Title says it all. Tuning advice, some evil. Not all will be applicable to Linux, but still contains knowledge that may prove useful</para></listitem>
		<listitem><para><ulink url="http://completelyfake.eu/illumos/docs/zfsadmin/">ZFS Administrator Guide</ulink> - THE guide to using ZFS </para></listitem>
		<listitem><para><ulink url="http://completelyfake.eu/"><citetitle>Completely Fake</citetitle></ulink>. My web page, where I write about things that make me tick. I am sure to write about ZFS now and then</para></listitem>
	</itemizedlist>
</para>
	<para>
		Apart from above, more or less static sources of information, you should visit irc #zfsonlinux channel on <ulink url="http://freenode.net">FreeNode IRC Servers</ulink> and <ulink url="http://zfsonlinux.org/lists.html">ZFS on Linux Mailing Lists</ulink>.
	</para>
</chapter>

<index />
</book>