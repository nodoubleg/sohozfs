<?xml version="1.0" encoding="utf-8"?>

<chapter xml:id="planning" xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0">
<title>Planning</title>
	<para>Before you go and buy a hardware for your storage, there are few things to consider. How much disk space will you need? How many client connections (sessions) your storage will serve? Which protocol will you use? What kind of data you plan to serve?
	</para>
<sect1 xml:id="planning-2"><title>Don't rush</title>
	<para>
		First advice that you always should keep in mind: don't rush it. You are about to invest your money and time. While you can later modify the storage according to your needs, some changes will require that you recreate the ZFS pool, which means all data on it will be lost. If you buy wrong disks (example: too small), you will need to add more and may run out of free slots. 
	</para>
</sect1>
<sect1 xml:id="planning-3"><title>Questionnaire</title>
	<para>There are few questions you should ask yourself before starting to scope the storage. Answers that you will give here will play key role in later deployment.
		<itemizedlist><listitem><para>The amount of data you expect to store will determine number and size of disks you will need to buy. That will also imply other factors, like server size.</para></listitem>
		<listitem><para>Number of concurrent client connections will determine amount of RAM that's you'll need. It may also imply buying SSD disks to serve as lever 2 cache for your storage and resigning from using SATA disk at all, if you were considering them. Even if you are going to store hundreds of terabytes, but only few client machines will ever utilize it and not very intensively, you may well get with low amount of memory.</para></listitem>
		<listitem><para>The above will also imply the kind of network interface in you server and switch it should be attached to.</para></listitem>
		<listitem><para>How critical the data are should push you into looking at certified and probably more costly hardware, known to perform well and for longer time. It will also tell you which redundancy level you should use, influencing the final cost.</para></listitem>
		<listitem><para>The kind of data you will serve, may imply the architecture of you storage pool. Streaming a video files for considerably large amount of clients or service virtual machines data files will most probably mean you need to use mirrors, which directly influence final capacity of your array.</para></listitem>
		</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="planning-4"><title>Scope</title>
	<para>
		Firstly, lets create upper bounds for what will be considered a SoHo storage in this guide. Given current disk sizes, up to 12 slots in a single node, up to 30 TB of raw capacity. Either internal SAS or SATA drives. One or two slots for eventual SSDs for speeding up reads. Possibly a mirrored ZIL device to speed up and concatenate writes to disks. A system drive, possibly mirrored, although, currently setting up Linux system on ZFS is not trivial and booting from ZFS is not recommended. Up to 128 Gigabytes of RAM, possibly 64. A 64bit CPU with four or more cores. While running ZFS on 32bit systems is possible, it certainly is not recommended. If you intend to use external disk enclosures (JBODS) connected through SAS or FibreChannel, this book is most probably not intended for you. It is possible to set up and administer such storage manually and many people have done so, but it may involve additional steps not covered in the guide. If you want to run tens or hundreds of disks, you'd probably do yourself a favor if you consider FreeNAS or even commercial solutions with paid support. Keeping track of system performance, memory usage, disks, controllers and cables health is probably best managed by specialized products.
	</para>
</sect1>
<sect1 xml:id="planning-5"><title>HW buyer guide</title>
	<para>
		Hardware is usually a long term investment. Try to remember about points below:
		<itemizedlist>
			<listitem><para>When buying disks, a common practice is to make sure you buy each disk from the same vendor and model, to keep geometry and firmware the same, but from different batch, so you minimize risk of several disks dying at the same time. I suppose that for a small time buyer (few up to 20 disks) simplest way to achieve it is to buy disks from different shops. A cumbersome, but storage operators have seen disk batches failing at the same time many times in their lives.</para></listitem>
			<listitem><para>Buy few pieces for spares. Storage system lifetime is usually counted in years and is often longer than that of disk model, especially if you decide to use consumer grate SATA disks. When one of them fails in few years, you may be surprised by the fact, that you cannot buy this model any more. Introducing different one in a pool is always a performance risk. Although, if that happens, don't despair. ZFS lets you exchange all disks in a pool. This trick has been used in the past to increase size of the pool, when it became insufficient. I'll explain the procedure in Maintenance chapter. Be aware, that replacing all disks in 10 disk pool can take weeks on a filled and busy system.</para></listitem>
			<listitem><para>Scope power supply properly. If your power unit is unstable or insufficient, you may encounter mysterious failures (disks dissapearing, disk connection dropping, random i/o errors to the pool) or may not be able to use your disks at all.</para></listitem>
			<listitem><para>Performance wise, the more disks the better. The smaller disks, the better. ZFS threads writes and reads among vdevs. The more vdevs, the more read/write threads.</para></listitem>
			<listitem><para>Performance wise, plan for much RAM. ZFS needs at least 2 Gigabytes of RAM to work sensibly, but for any real life use, I'd not go below 8 Gigabytes. For a storage system for SoHo, I would recommend looking at 64 GiB and more. The thing is, ZFS caches data very aggressively, so it will try to use as much RAM as possible. It will however yield any time, system demands RAM for normal operation (new program being run etc.). So the more it can fit in your memory, the better. </para></listitem>
			<listitem><para>Plan for SSDs (at least three). You don't need to buy them upfront. ZFS is a hybrid storage filesystem, which means, it can use SSD disks for level 2 cache. It's gonna be much slower than you RAM, but it's cheaper and still much faster than your platter disks. For a fraction of RAM price you can get 512 GiB SSD drive, which should allow for another speed improvement. That's one SSD. Next two SSDs would be for external ZFS Intent Log. The filesystem don't flush all data all the time to physical storage. It ties writes in transactions and flushed several at the same time, to minimize filesystem fragmentation and real i/o to disks. If you give ZFS external device for ZIL, it can speed things up by grouping even more data before flushing it down. This additional pool device should be mirrored, because it's where you can loose your data. In case of power failure, data on external ZIL must be persistent. There are battery backed up DRAM devices that emulate small SSD disks, ie. ZeusRAM. They come in 8 and 16 GB sizes, which is enough for ZIL and are fast as memory, but they are costly. You can think of mirroring your L2ARC too (the level 2 cache), but loosing this device won't endanger your data.</para></listitem>
			<listitem><para>While SAS standard is sure to get you better performance and life expectancy from your disks, for SoHo solutions SATA is enough, especially if you consider that there are enterprise class SATA disks. The price difference however for such deployment shouldn't be very high. If unsure, choose SAS, if your budget allows.</para></listitem>
			<listitem><para>Do not buy hardware and softraid controllers. While in the past RAID cards were necessary to offload both CPU units and RAM, both of those resources are now abundant and cheap. You CPU and your RAM will be more than enough for the workload and RAID cards take away one important capability of ZFS. ZFS ensures data safety by talking directly to the disk: getting reliable information on when data i s flushed to physical disks and what block sizes are being used. RAID controllers mediate in between and can make their own "optimizations" to the I/O, which may lower ZFS reliability. Other thing is, RAID controllers are incompatible between various vendors and even the same card but different firmware revision may be unable to access your RAID set. This means, that in case of controller failure, you loose whole setup and need to restore data from backup. Softraids are even worse, in that that need a special software (often limited to only one operating system), to actually work. ZFS is superior in all of above. Not only can it use all processing power and all RAM you can give it to speed up your I/O, but also disks in the pool can be migrated between all software platforms that implement the same OpenZFS version. Also, the exact sequence of disks in disk slots is not important, as pool remembers its configuration based on both disk device names (ie. /dev/sdb) but also disk GUID given them by ZFS during pool creation.</para></listitem>
			<listitem><para>Networking cards at least 1Gb of speed. Remember, that this server networking card's bandwidth will be spread among all computer machines that will simultaneously utilize the storage. It is quite sensible to consider 10Gb, but you also need to consider your other networking gear: switches, cabling etc. Remember though: network plays a role in performance analysis at quite large amount of performance issues I was debugging were caused not by storage itself, but by networking layer.</para></listitem>
			<listitem><para>Plan for redundancy. Always. This means, that for high speed read pools you need to consider mirrored storage, effectively halving total capacity of disks you buy. RAIDZ setup means your capacity will be lowered by one disk per each vdev you create. For RAIDZ-2, it will be two disks.</para></listitem>
		</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="planning-6"><title>Data security</title>
<para>
	You are going to use ZFS storage to keep data and serve it to various people in your company. Be it two, ten or fifty, always put some thought into planning the layout. Various directories that will store data that vary by kind, sensitivity, compressibility will pay off in the future. Well designed directory structure will simplify both organizational things, like access control, and technical side, like enabled or disabled compression, atime option etc.
</para>
<para>
	ZFS filesystems behave like directories. It is quite common to create a separate ZFS filesystem per user home directory, or example, so that they can have fine grained backup policies, ACLs and compression mechanisms.
</para>
<para>
	This is one place in the guide, where I cannot be very specific. You will need to consider your company size, number of employees accessing the storage, growth perspectives, data sensitivity etc. Whatever you do, however, don't skip this point. I've seen quite a few companies, which overlooked the moment they have should switched from infrastructure that freely evolves, into something that is engineered. 
	</para>
	<sect2 xml:id="planning-7"><title>CIA</title>
	<para>
		There are many data security methodologies and one of them, I believe most classic, uses acronym C.I.A. to explain aspects of data security. Letters stand for Confidentiality, Integrity and Availability. While it focuses rather on InfoSec side of things, it's pretty good view on storage administration also. Let me introduce those in the view of storage administrator.
		<itemizedlist>
			<listitem><para>Confidentiality. Data must be available only to people that are entrusted with them. No one that is not explicitly allowed to view data, shouldn't be able to access it. This side of security is covered by many infrastructural tools, from policies and NDAs that people allowed to view data should read and sign, through network access separation (VPNs, VLANs, access control through credentials), there are also aspects directly related to storage itself: Access Control Lists (ACLs), sharing through secure protocols and in secure networks, working with storage firewall etc.</para></listitem>
			<listitem><para>Integrity. It must be guaranteed that data is genuine and was not changed by people that are not entrusted. Also, the change should not be introduced by software or hardware, intentionally or not, if it's not supposed to do it. Through whole data lifecycle, only people with sufficient privileges should be allowed to modify data. Unintentional data integrity breach may be a disk failure that breaks data blocks. While with text data it is usually easily spotted, with other data, like sound or video, that may introduce subtle differences from original state. As with all aspects of security, it's also only partially administered by storage. The data integrity is covered by ACLs, but also by ZFS checksumming data blocks to detect corruption. If your setup uses any redundancy, ZFS is able, to great extent, fix those for you using the redundant set. </para></listitem>
			<listitem><para>Availability. The data should be available at all times it is required and guaranteed. This is probably one of most obvious aspects of storage. At any time that you expect your data should be up, the data should be up. Typically storage redundancy is brought up here (mirror, raidz, raidz2), but also network cards trunking, switch stacking and redundancy of any credentials checking solution you may be using (Active Directory server, primary and secondary, for example).</para></listitem>
		</itemizedlist>
		Through this guide I will cover some of those three related to storage platform. I won't however, cover in depth administration of Linux firewall and networking, as those are subjects for separate books. Remember though, while C.I.A triad go introduced in the layout section, it applies to any other aspect of the storage. From writing access policies, through ensuring hardware is working properly, to configuring the system so that it does perform well enough so that data is available to all interested parties at any time guaranteed. 
	</para>
</sect2>
</sect1>
<sect1 xml:id="planning-8"><title>Types of workload</title>
	<para><indexterm><primary>L2ARC</primary></indexterm>
		The workload you are going to run on the storage will play major role in how you should plan the pool layout.
		<itemizedlist>
			<listitem><para>If you are going to mostly host databases and they are going to be dominating consumers of the space, L2ARC SSD device may not provide you with special performance gains. Databases are very good at caching their own data and if it so happens that the data fits into database server RAM, ARC will not have much to do. On the other hand, if the data in your database change often and will need to be reread from disks anyway, you are going to have high miss ratio anyway and again L2ARC device will not fulfill its purpose. The snapshotting data is also going to be tricky. Databases need lots more than a snapshot of filesystem to be able to work on the data. This is why they come with their own dump commands - because the full working backup will contain more than what lives in database files usually. Hosting database would usually mean you run the engine on the same host than your ZFS. Again, Database will use the RAM probably more efficiently than the filesystem itself. Consider though, if you will serve data from the same server for other purposes, like CIFS or NFS share, database and filesystem cache may compete for RAM. While they shouldn't affect the system stability, it may adversely affect he performance.</para></listitem>
			<listitem><para>If you host documents and pictures for office workers, files like procedures, technical documentations, L2ARC device is something to seriously consider. Snapshotting is then a reliable way of capturing data at certain point of time. If your data are not being accesses 24 hours a day and you can have just few seconds of off time, when no one is going to be editing them, a snapshot usually takes about a second to create and will reliably host your data at a specified point of time. You can later mount this snapshot - remember it is read only - and transfer to a backup location not worrying about data integrity.</para></listitem>
		</itemizedlist>
		Above all, don't rush it though. You can always add L2ARC later on to your pool, if performance tests prove to be unsatisfactory. 
	</para>
	<para>
		For various workload types various pool configurations are best suited. Lets first introduce terms describing storage performance:
		<indexterm><primary>IOPS</primary></indexterm>
		<indexterm><primary>storage bandwidth</primary></indexterm>
		<itemizedlist>
			<listitem><para>IOPS - it's Input/Output Operations Per Second. It is ability of hard drive or of whole storage array to write/read number of blocks per second. The IOPS needs maximum block size the disk or storage is able to write/read.</para></listitem>
			<listitem><para>Storage bandwidth is ability of disk or storage to pass amount of data per second, usually measures in MB per second.</para></listitem>
		</itemizedlist>
		Important thing to understand is that various redundancy provide various speeds for both writes and reads. ZFS pool writes are as fast as the slowest vdev and a vdev is as fast as slowest disk in it. It is due to the internal implementation of ZFS.
	</para>
	<para>
		This guide will not delve into IOPS and bandwidth mechanics. If you wish to understand it more, there are reliable online resources you can study, with <ulink url="https://en.wikipedia.org/wiki/IOPS">Wikipedia page</ulink> as a starting point.
	</para>
	<para>
		For your purposes, lets only remember, that mirrored vdevs increase read performance with each added disk. If you have two-way traditional mirrors, then your read performance is doubled. If you create a triple mirrors - it's increased three times. If you have a pool consisting of two vdevs created from three mirrored disks, then your performance can peak to a six times single disk speed. Keep in mind however, that this is given all the read data are spread between both vdevs - not uncommon, since ZFS tries to balance data spread among all vdevs it currently has. Write speed however is going to be another matter. No matter the number of disks on the pool, your performance is going to be equal the speed of slowest disk in the pool. This makes mirrors perfect for streaming or hosting virtual machines images.
	</para>
	<para>
		For RAIDZ however, the read speeds will be much slower than for mirrors. While it is not going to be exactly the speed of a single disk, the increase is going to be much slower than with mirrors. RAIDZ will however outperform mirror in writes, since each disk will only receive a chunk of data and thus the write will be done much faster than with full data set that mirrors must receive.
	</para>
</sect1>
<sect1 xml:id="planning-9"><title>Pool layout</title>
	<para>
		We've already presented various pool layout performance. Lets now consider rules of thumb for given redundancy types.
		<itemizedlist>
			<listitem><para><indexterm><primary>ZFS</primary><secondary>stripe</secondary></indexterm>There is one pool layout that was not introduced so far and for a good reason. Striped pool is a pool consisting of two or more disks that provide no redundancy. While the total pool capacity equals combined capacity of all disks within the pool, the filesystem will become corrupted and subject to data recovery given loss of a single drive. A rule of thumb for storage is: don't use stripe.</para></listitem>
			<listitem><para><indexterm><primary>ZFS</primary><secondary>mirror</secondary></indexterm>For mirrored pool a rule of thumb is, to use it only when you really need an incredible read performance or are paranoid about your storage. Reason is that disks don't fail that often and mirrors half your total pool capacity. With triple mirrors capacity will be total disks capacity divided by three. Rule of thumb - use sparingly and with care.</para></listitem>
			<listitem><para><indexterm><primary>ZFS</primary><secondary>raidz</secondary></indexterm>Rule of thumb for RAIDZ (which are a rough equivalents of RAID-5 and RAID-6) is to go rather for RAIDZ-2. It gives you quite good resilience with conserving a lot of space. There is also another recommendation and from personal experience I'd adhere to it: for RAIDZ pools have 2n+1 disks per vdev. That's three, five, seven etc., but no more than eleven. This is 2n data disks plus 1 disk for parity data. With the smallest set of three disks per vdev you have basically a capacity of mirrored set with lower read performance. Consider starting from five disks per vdev. For RAIDZ-2, the rule is to use 2x+2 disks, which translates to four, six, eight etc and have no more than twelve disk within a vdev. Given this guide considers typical target maximum of twenty disks in the pool (including ZIL and L2ARC), a good idea would be to have two eight disks RAIDZ-2 vdevs in the pool, totalling sixteen disks of total pool capacity of twelve disks. </para></listitem>
		</itemizedlist>
	</para>
</sect1>
<sect1 xml:id="planning-10"><title>Other components to pay attention to</title>
	<para>
		It is important to pay attention to other infrastructure elements. Network would be of special interest. In a small company of few persons a small switch with workstation refit as a storage server both elements may perform without any issue, but once the number of data consumers start to grow, the network may soon turn to be a bottleneck. Switches may not be only limiting factor. Network cards in your storage server may prove to be another one. Also, if you serve your data over VPN from remote location, it may turn out the interlink is too slow. Quite often on a storage performance analysis case we were able to point networking infrastructure as a faulty element. 
	</para>
</sect1>
</chapter>