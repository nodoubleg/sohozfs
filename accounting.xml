<?xml version="1.0" encoding="utf-8"?>

<chapter xml:id="accounting" xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0">
	<title>Space accounting</title>
<sect1 xml:id="accounting-1"><title>Understanding space accounting in ZFS</title>
<para>
	Due to rich feature set, some relying on filesystem organization, the likes of clones, snapshots and compression, space monitoring needs to be done differently from traditional Linux filesystems. The usual <command>df <option>-h</option></command> command known to each Linux server administrator is no longer sufficient and may even be misleading. You should learn to use two commands and understand their arguments and output to keep track of your free space: <command>sudo zpool list</command> and <command>sudo zfs list</command>, which, on my home workstation commands will produce output as below:
 	<programlisting>
  	<computeroutput>
  	trochej@madchamber:~$</computeroutput> <userinput>sudo zpool list</userinput>
  	<computeroutput>
	NAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
	data  2,72T   147G  2,58T         -     3%     5%  1.00x  ONLINE  -
	</computeroutput>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list</userinput>
	<computeroutput>
	NAME          USED  AVAIL  REFER  MOUNTPOINT
	data          147G  2,53T    96K  /data
	data/datafs   147G  2,53T   147G  /mnt/data
	</computeroutput>
	</programlisting>
	This may come as a surprise, but the list about is not complete. This is because list by default omits snapshots. Remember, snapshots consume space increasingly with time, as data change on the snapshotted system. One of very often issues raised by new ZFS storage operators was that they were unable to delete data due to lack of space. They were baffled by both the fact, that deleting data wouldn't increase the available space and that consumed space in ZFS list wouldn't add up to total space available int the pool.
</para>
<para>
	It is sometimes a bit difficult to understand what consumes your pool space. I will try to explain on examples, but nothing beats experience. Create a pool, fill it with data, run snapshots, delete, create reservations. All the time observe <command>zfs list<option> -t all -o snapshot</option></command> and <command>zfs list <option>-t all</option></command> to better understand the space accounting.
	</para>
<para>
	Lets consider a situation when you have a 3 TB pool.
 	<programlisting>
  	<userinput>
	sudo zpool create datapool mirror /dev/sdb /dev/sdc
	sudo zfs create datapool/data
 	</userinput>
 	</programlisting>
 	After successful import of 2 TB of backed up data, you decide to create snapshot, so that users mistakenly deleting data wouldn't make you rerun the backup restore.
 	 <programlisting>
  	<userinput>
	sudo zfs snapshot datapool/data@after-backup-restore
 	</userinput>
 	</programlisting>
 	Take a note that running this snapshot is instantaneous and takes no disk space initially.
 </para>
 <para>
 	As it sometimes happen, just after you ran the snapshot, a user with very wide access rights (CEO maybe?), really deletes whole 2 TB of data. But, the delete job stops short of 1 TB with information, that it cannot delete more, due to the lack of space. How is that even possible? Answer is: the snapshot.
</para>
<para>
	Lets first observe filesystem on my workstation:
	<programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list</userinput>
	<computeroutput>
	NAME          USED  AVAIL  REFER  MOUNTPOINT
	data          134G  2,55T    96K  /data
	data/datafs   134G  2,55T   134G  /mnt/data
 	</computeroutput>
 	</programlisting>
 	Now, let me create a snapshot there:
	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs snapshot data/datafs@testsnapshot</userinput>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       134G  2,55T    96K  /data
	data/datafs                134G  2,55T   134G  /mnt/data
	data/datafs@testsnapshot      0      -   134G  -
 	</computeroutput>
 	</programlisting>
		Let me now upload a CentOS 7 GB iso file to the <literal>/mnt/data</literal>:
 	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       141G  2,54T    96K  /data
	data/datafs                141G  2,54T   134G  /mnt/data
	data/datafs@testsnapshot  7,14G      -   134G  -
 	</computeroutput>
 	</programlisting>
 	Notice that the snapshot size has increased up to the newly introduced data. Let me now delete whole directory containing archived ISOs:
 	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       141G  2,54T    96K  /data
	data/datafs                141G  2,54T   109G  /mnt/data
	data/datafs@testsnapshot  32,0G      -   134G  -
 	</computeroutput>
 	</programlisting>
 	What you will see is that while the REFER size for the data/datafs ZFS filesystem has shrunk, the overall USED stays the same and snapshot size has increased up to 32 GB. For comparison, lets have a look at the <command>df <option>-h</option></command> command (I have removed non-ZFS filesystems from output for clarity:

 	 <programlisting>
  	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>df -h</userinput>
	<computeroutput>
	Filesystem      Size  Used Avail Use% Mounted on
	data            2,6T  128K  2,6T   1% /data
	data/datafs     2,7T  109G  2,6T   5% /mnt/data
 	</computeroutput>
 	</programlisting>
 	Let me now remove some more data from datafs, just to increase the size of the snapshot:
 	<programlisting>
 	<computeroutput>
 	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
 	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       141G  2,54T    96K  /data
	data/datafs                141G  2,54T  23,3G  /mnt/data
	data/datafs@testsnapshot   117G      -   134G  -

	trochej@madchamber:~$</computeroutput> <userinput>df -h</userinput>
	<computeroutput>
	Filesystem      Size  Used Avail Use% Mounted on

	data            2,6T  128K  2,6T   1% /data
	data/datafs     2,6T   24G  2,6T   1% /mnt/data
 	</computeroutput>
 	</programlisting>
 	As you may notice, there is not much to be gleaned from the du command. It more or less tracks the space usage, but it tells nothing about the pattern. The zfs list on the other hand, tells you quite a lot. By this output alone you can see, that while your filesystem used space has shrunk, the overall used stays the same, it's just moved into another dataset's location. </para>
 	<para>
 		The <command>zfs</command> can provide you with even deeper understanding of how the space is distributed among your data. And while it's not very interesting in the case of the small experiment I've been running so far, I'll provide you with more complicated examples in just a moment. First, howewer, lets check out another option to the <command>zfs list</command>:
 	 	 <programlisting>
  			<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all -o space</userinput>
	<computeroutput>
	NAME                      AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD
	data                      2,54T   141G         0     96K              0       141G
	data/datafs               2,54T   141G      117G   23,3G              0          0
	data/datafs@testsnapshot      -   117G         -       -              -          -
 	</computeroutput>
 	</programlisting>
 	More detailed explanation of -o space follows in next section. 
</para>
<para>
	It should be now pretty clear where the issue with data deletion came from. Since the 3TB pool is capable of keeping more or less the amount of data (modulo data compression), introducing deletion of 2 TB of data on a filesystem that already holds 2 TB results in pool space running out, since the pool needs to add data to snapshot as the user keeps removing them.
</para>
<para>
	As a side note, I was whole the time working on my production data, and while I keep backups, I was confident enough in the snapshot to let me do this:
	<programlisting>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs rollback data/datafs@testsnapshot</userinput>
	<computeroutput>
	trochej@madchamber:~$</computeroutput> <userinput>sudo zfs list -t all</userinput>
	<computeroutput>
	NAME                       USED  AVAIL  REFER  MOUNTPOINT
	data                       134G  2,55T    96K  /data
	data/datafs                134G  2,55T   134G  /mnt/data
	data/datafs@testsnapshot     8K      -   134G  -
	</computeroutput>
	</programlisting>
</para>
</sect1>
<sect1 xml:id="accounting-2"><title>zfs -o space</title>
<para><command>zfs list <option>-o space</option></command> command and its output is important enough to warrant it a separate subsection in this guide. 
</para>
<para>
The example we went through above is pretty simple. Not much is happening on this pool and not many additional features were used. I will then create a sample ZFS pool using files storage (files emulating real block devices) and we will play few scenarios to see how setting up various ZFS properties affects available space and the zfs -o space output.
</para>
<para>
	But first, lets go through all the columns in the output and what they mean to operator:
	<itemizedlist>
		<listitem><para>AVAIL - available - total available space in the filesystem</para></listitem>
		<listitem><para>USED - used - total used space in the filesystem</para></listitem>
		<listitem><para>USEDSNAP - usedbysnapshots – the disk space used by snapshots of the dataset. This space would be freed once all snapshots of the dataset are destroyed. Since multiple snapshots can reference the same blocks, this amount may not be equal to the sum of all snapshots used space</para></listitem>
		<listitem><para>USEDDS - usedbydataset – the disk space used by the dataset itself. This disk space would be freed if: all snapshots and refreservation s of this dataset were destroyed and then the dataset itself would be destroyed</para></listitem>
		<listitem><para>USEDREFRESERV - usedbyrefreservation – the disk space used by a refreservation set on the dataset. This space would be freed once refreservation is removed</para></listitem>
		<listitem><para>USEDCHILD - usedbychildren – the disk space used by children of the dataset. This space would be freed after destroying children of given dataset</para></listitem>
	</itemizedlist>
	To calculate the USED property by hand, follow equation below:
	<literal>USED = USEDCHILD + USEDDS + USEDREFRESERV + USEDSNAP</literal>
</para>
	<para>
		The <command>zfs <option>-o space</option></command> is not very informative and interesting in the very simple example above. Consider however a following configuration:
		<itemizedlist>
			<listitem><para>A pool named datapool with RAID-Z2 redundancy</para></listitem>
			<listitem><para>5 filesystems within, two of which have regular snapshots taken each hour and retained for two weeks. Every Saturday a snapshot is taken, retained for a month.</para></listitem>
			<listitem><para>2 of filesystems above have quota set</para></listitem>
			<listitem><para>1 filesystem have set reservations</para></listitem>
			<listitem><para>1 zvol created</para></listitem>
		</itemizedlist>
		Lets the this configuration in print:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zpool list
	NAME       SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
	datapool  23.8G  5.37G  18.4G         -    14%    22%  1.00x  ONLINE  -
		</programlisting>
		So the pool says, there is above 18 Gigabytes space free in the pool. Lets look closer:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list
	NAME              USED  AVAIL  REFER  MOUNTPOINT
	datapool         13.2G  2.41G  34.0K  /datapool
	datapool/first   3.58G  6.83G  3.58G  /datapool/first
	datapool/five    50.0K  2.41G  32.0K  /datapool/five
	datapool/fourth  50.0K  2.41G  32.0K  /datapool/fourth
	datapool/second  50.0K  2.41G  32.0K  /datapool/second
	datapool/third   50.0K  2.41G  32.0K  /datapool/third
	datapool/vol01   5.16G  7.57G  16.0K  -
		</programlisting>
		But not exactly. Shouldn't AVAIL number be the same as FREE in <command>zpool list</command> output? After all it is being said, ZFS filesystems can grow up to the pool's capacity. Lets list <emphasis>all</emphasis> datasets then:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list -t all
	NAME                               USED  AVAIL  REFER  MOUNTPOINT
	datapool                          13.2G  2.41G  34.0K  /datapool
	datapool/first                    3.58G  6.84G  3.58G  /datapool/first
	datapool/first@2016-02-17-14:55   18.0K      -  32.0K  -
	datapool/first@2016-02-17-15:04       0      -  3.58G  -
	datapool/five                     50.0K  2.41G  32.0K  /datapool/five
	datapool/five@2016-02-17-14:55    18.0K      -  32.0K  -
	datapool/fourth                   50.0K  2.41G  32.0K  /datapool/fourth
	datapool/fourth@2016-02-17-14:55  18.0K      -  32.0K  -
	datapool/second                   50.0K  2.41G  32.0K  /datapool/second
	datapool/second@2016-02-17-14:55  18.0K      -  32.0K  -
	datapool/third                    50.0K  2.41G  32.0K  /datapool/third
	datapool/third@2016-02-17-14:55   18.0K      -  32.0K  -
	datapool/vol01                    5.16G  7.57G  16.0K  -
		</programlisting>
		Okay. So there are snapshots in play, so it might have taken some of the capacity, but still, why numbers are different among the datasets? Lets first explain the <literal>REFER</literal> column in <command>zfs list</command> output. It states, how much space the dataset is keeping references to. See that in the output above:
		<programlisting>
	datapool/first@2016-02-17-15:04       0      -  3.58G  -
		</programlisting>
		<literal>USED</literal> column is zero, but <literal>REFER</literal>is above 3.5G. That is typical to snapshots. Since the creation of the snapshot no change was introduced to the filesystem datapool/first, thus snapshot does not use any space at the moment. But it keeps references to 3.5 G of data that datapool/first contained at the time of snapshotting. Lets make it use some space now by removign a piece of data I copied over to the datapool:
		<programlisting>
	trochej@ubuntuzfs:~$ rm /datapool/first/Fedora-Live-KDE-x86_64-23-10.iso 
		</programlisting>
	Check how it looks like right now:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list
	NAME                               USED  AVAIL  REFER  MOUNTPOINT
	datapool                          14.7G   930M  34.0K  /datapool
	datapool/first                    9.50G  4.91G   741M  /datapool/first
		</programlisting>
		So, the filesystem datapool/first consumes 9.5G of space, fur references 741M only? Where is the rest of the claimed space consumption? First, run zfs list -t all, to see not only filesystems, but snapshots also:
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list -t all
	NAME                               USED  AVAIL  REFER  MOUNTPOINT
	datapool                          14.7G   930M  34.0K  /datapool
	datapool/first                    9.50G  4.91G   741M  /datapool/first
	datapool/first@2016-02-17-14:55   18.0K      -  32.0K  -
	datapool/first@2016-02-17-15:04   18.0K      -  3.58G  -
	datapool/first@2016-02-17-15:22   1.20G      -  5.50G  -
	datapool/first@2016-02-17-15:27       0      -   741M  -

	trochej@ubuntuzfs:~$ ls -ahl /datapool/first/
	total 741M
	drwxr-xr-x 2 trochej trochej    3 Feb 17 15:25 .
	drwxr-xr-x 7 trochej trochej    7 Feb 17 14:51 ..
	-rw-r----- 1 trochej trochej 741M Feb 17 15:21 FreeBSD-11.0-CURRENT-amd64-20151130-r291495-disc1.iso
		</programlisting>
		Okay. So the filesystem holds 741M of data, but its snapshots consume 1.20GB of space. That's more like it. Still, where's the rest of my space gone?
		<programlisting>
	trochej@ubuntuzfs:~$ sudo zfs list -t all -o space
	NAME                              AVAIL   USED  USEDSNAP  USEDDS  USEDREFRESERV  USEDCHILD
	datapool/first                    4.91G  9.50G     4.78G    741M             4G          0
	datapool/first@2016-02-17-14:55       -  18.0K         -       -              -          -
	datapool/first@2016-02-17-15:04       -  18.0K         -       -              -          -
	datapool/first@2016-02-17-15:22       -  1.20G         -       -              -          -
		</programlisting>
		The output is cut out for brevity. The datapool/first filesystem consumes 4.78G in snapshots. 4G is used by refreservation property set on the filesystem, giving it 4G of free space at cost of other filesystems. Reservations have been explained in <xref linkend="advsetup-5" />.

	</para>
</sect1>
</chapter>